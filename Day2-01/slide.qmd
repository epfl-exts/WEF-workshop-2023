---
title: Bayesian AB Testing 
subtitle: Bernoulli and Value Conversions
author: EPFL Extension School
institute: World Economic Forum
date: 2023-09-29
editor:
  render-on-save: false
format:
  revealjs:
    auto-stretch: false
    code-copy: true
    code-link: true
    incremental: false
    embed-resources: true
    smaller: true
    scrollable: false
    theme: ../assets/theme.scss 
    # theme: simple
    logo: ../assets/img/logo_red.png
    code-fold: true
    code-line-numbers: false
    slide-number: true
    # chalkboard:
    #   buttons: true
    preview-links: auto
    menu: false
title-slide-attributes:
  data-background-image: img/AdobeStock_430543026.jpeg
  data-visibility: hidden
from: markdown+emoji
execute:
  enabled: true
  cache: true
jupyter: python3
---


## {chalkboard-buttons="false" background-image="img/AdobeStock_430543026.jpeg"  background-size="cover" }

## {chalkboard-buttons="false" background-image="../assets/img/AdobeStock_536776996.jpeg"  background-size="cover" }

 
[Goal of This Session:]{style="font-size: 1.5em; font-weight: bold;"}

<br>


:heavy_check_mark: Python

:heavy_check_mark: Statistical Distributions & Inference

:heavy_check_mark: Bayesian Statistics

:heavy_check_mark: Bayesians vs Frequentists

:heavy_check_mark: AB Testing

&#9744; [AB Testing in Python]{style="color:#ff412c;"}

## AB Testing


::: columns
::: {.column width="60%" }
- We have a website and we want to increase the number of `visitors` that make a subscription
- We have two versions of the website: `A` and `B`
:::
::: {.column width="40%" }
![](img/AdobeStock_532613325.jpeg){width="70%" fig-align="left"}
:::
:::

::: columns
::: {.column width="60%" }
- We want to know which version leads to more `subscriptions`
- We can use AB testing to compare the two versions  
::: 
::: {.column width="40%" }
![](img/AdobeStock_120197715.jpeg){width="65%" height="65%" fig-align="left"}
:::
:::

::: columns
::: {.column width="90%" }
::: callout-note
- Here the target action is the `subscriptions`, but we can generalize this to any action that we want to increase, e.g. clicks, purchases, likes, time spent on the website, etc. 
- A visitor that makes a subscription is called a `conversion`
:::
::: 
::: {.column width="1%" }

:::
:::

::: notes
A/B testing is a method of comparing two versions of a 'product' to determine which one performs better
:::



## Setting up the Model

<br>


::: columns
::: {.column width="70%" }

#### Parameters

- $\theta_A$ is the conversion probability for version A
- $\theta_B$ is the conversion probability for version B
<!-- - $\frac{\theta_B}{\theta_A}-1$ is the uplift of version B compared to version A -->
:::
::: {.column width="30%" }
![](img/AdobeStock_169591536.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::

<br>

#### Priors
::: columns
::: {.column width="70%" }


-  Conversion probabilities are Beta distributed:
$$\theta_A \sim Beta(\alpha_A, \beta_A)$$
$$\theta_B \sim Beta(\alpha_B, \beta_B)$$
-  For simplicity we assume $\alpha_A=\beta_A=\alpha_B=\beta_B$
::: 
::: {.column width="30%" }
![](img/AdobeStock_105107192.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::





## Setting up the Model

<br>

::: columns
::: {.column width="70%" }
#### Likelihoods
- Both versions get `visitors` 
- `conversion` is Bernoulli distributed with conversion probability $\theta$ 
:::
::: {.column width="30%" }
![](img/AdobeStock_124620223.jpeg){width="75%" fig-align="left"}
:::
:::

<br>

#### Posteriors

::: columns
::: {.column width="70%" }
- Remember that in this case posteriors are also Beta distributed, thanks to the conjugate property
- We can get them via sampling methods too. So no worries if we can't benefit from the conjugate property 
::: 
::: {.column width="30%" }
![](img/AdobeStock_109501465.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::




::: notes
visitors are like the trials or coin flips in the Bernoulli distribution. We have 1000 trials for each variant.
conversions are like the successes or number of heads in the Bernoulli distribution. We have 200 successes for each variant.

Each visitor action to `convert` or not is Bernoulli distributed with conversion probability $\theta$ 

:::

## Remarks

<br>

::: {.callout-note title="Notes" appearance="simple" icon=true} 
- Priors can follow other distributions than Beta
- Priors can be different for each version
- Number of visitors can be different for each version
- Conversions are happenning independently
:::



![](img/AdobeStock_601177870.jpeg){width="75%" height="75%" fig-align="center"}


## What Do We Want to Know?

<br>
Our goal is to find out which version is better, and that's why we need the `uplift` $\frac{\theta_B}{\theta_A}-1$:

- if the `uplift > 0`, then version B is better
- if the `uplift < 0`, then version A is better
- if the `uplift = 0`, then there is no difference between the two versions

<!-- ::: {style="text-align: center"}
::: box
Luckily we get the whole distribution of the uplift, not just these three states
:::
::: -->
::: fragment
::: callout-tip
Luckily we get the whole distribution of the uplift, not just these three states ðŸ˜ƒ
:::
:::

<!-- ## ![](https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg){width="150px" height="150px" fig-align="left"} -->
<br>

::: fragment
::: columns
::: {.column width="60%" }
This is where we need PyMC ðŸŒ±, a Python library for Bayesian statistical modeling, and we use it to do the Bayesian inference for the AB testing problem.

We can simply import it by:

```{.python}
import pymc as pm
```
:::

::: {.column width="40%" }
![](https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg){width="95%" height="95%" fig-align="left"}
:::
:::
:::




```{python}
#| echo: false
#| eval: true

import matplotlib.pyplot as plt
import pandas as pd
import arviz
import numpy as np
import pymc as pm
rng = np.random.default_rng(4000)
__spec__ = None
plotting_defaults = dict(
    bins=50,
    kind="hist",
    textsize=8,
)
```



::: notes
is a python library for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems. Along with core sampling functionality, PyMC includes methods for summarizing output, plotting, goodness-of-fit and convergence diagnostics.
:::


## Define Parameters in ![](img/PyMC.png){.center}

```{python}


```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2|4,5|7,8|10,11

# let's define the variants A and B
variants  = ['A', 'B']

# let each variant to have 1000 trail 
visitors    = [1000, 1000]

# 200 of which leads to success
conversion = [200, 200]

# let's define parameters for a weak prior for the conversion rates
weak_alpha, weak_beta = [100, 100]
```

```{python}
#| echo: false
#| eval: true
#| code-fold: false
from scipy.stats import beta

# plot probability distribution
x = np.linspace(0,1,100)
fig, ax = plt.subplots(figsize=(5, 3))
plt.plot(x, beta.pdf(x, weak_alpha,weak_beta), 'r-', lw=2, )
plt.xlabel(r'$\theta$')
plt.title(r'$\beta(100,100)$ PDF')
plt.show()
```



## Creating a Model in ![](img/PyMC.png){.center}

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1|4|5,6|7|10|11,12|4,13|7,14|4,17,18|21
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n        = visitors, 
                      observed = conversion, 
                      p        = theta,
                      shape    = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the posterior
    trace = pm.sample(draws=1000, return_inferencedata=False, progressbar=False)
```

![](img/Screenshot%202023-07-16%20at%2016.12.14.png){width="80%" height="80%" fig-align="left"}

::: notes
how to explain the with statement in pymc?

the pymc code looks like this: it hijacks the context manager which one usually uses for 
opening/closing files and instead uses it to create a context for the model and define its variables. Here it opens a model (WITH pm.Model() AS model)
and populate it with the priors and variables. Then it runs the model and samples from the posterior. 
Model is an object which is a container  for the model random variables. It is the top level container for all probability models. 

Finally it returns the posterior samples. The context manager is a python construct that
allows you to do something before and after a block of code. In this case it opens the model
and closes it after the sampling is done.
Any time you declare a pymc mvariable inside a with statement (or context manager), it gets added to the model. Pymc is a high level language.

chain: Each row of an array is treated as an independent series of draws from the variable, called a chain
:::

## Checking the Model Specification

Unobserved Random Variables:

```{python}
#| echo: true
#| eval: true
#| code-fold: false
example_model.unobserved_RVs  
```

<br>

Observed Random Variables:

```{python}
#| echo: true
#| eval: true
#| code-fold: false
example_model.observed_RVs  
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false

trace
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.theta.shape,  trace.uplift.shape
```


## Ploting the Output



```{python}
#| echo: true
#| eval: true
#| code-fold: false
# let's put the trace into a dataframe
weak_outcome = pd.concat([pd.DataFrame(trace['theta']), 
                          pd.DataFrame(trace['uplift'])],
                          axis=1)
weak_outcome.columns = ['theta_A','theta_B','uplift']
```

<br>

::: panel-tabset
### plot
```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 5,9,13
# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True, sharex=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift
ax[2].hist(weak_outcome['uplift'], bins=50,)
ax[2].set_title('uplift')

plt.show()
```

### show it
```{python}
#| echo: false
#| eval: true


# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True, sharex=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift
ax[2].hist(weak_outcome['uplift'], bins=50,)
ax[2].set_title(r'uplift: $\frac{\theta_B}{\theta_A}-1$')

plt.show()
```

:::

## Changing the Output Type

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 22,23
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n = visitors, 
                      p = theta, 
                      observed = conversion,
                      shape = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the prior
    trace = pm.sample(draws=1000, 
                      return_inferencedata=True)
```

## Changing the Output Type

Let's check the output:

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace
```

::: callout-tip
The output type is different, it is now a `InferenceData` object. 
:::

<br>

It is easy to use the information in the object:

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.posterior.theta.shape
```

::: notes
as an example, for theta we have 4 chains, each with 1000 draws. So for theta_A we have 4 chains, each with 1000 draws.

explain the InferenceData object
:::


## Exploring the Posterior

<br><br>


```{python}
#| echo: true
#| eval: true
#| code-fold: false

pm.summary(trace, var_names=["theta", "uplift"]).loc[:, ["mean","sd", "hdi_3%", "hdi_97%"]]
```

- DISCUSS THE OUTPUT

::: notes
did you expect to have 0.25 as the mean for theta_A? why?

what is HDI?
:::

## Exploring the Posterior

<br>

We can also use the `ArviZ` library for plotting. The main advantage is that it works pretty well with the output data. Also, it gives the HDI for the Relative Uplift distribution.

<br>

::: columns
::: {.column width="60%" }
```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,4
arviz.plot_posterior(trace.posterior["uplift"], 
                    **plotting_defaults, 
                    figsize=(5, 3))

plt.title("B vs. A Relative Uplift Distribution", fontsize=10)
plt.axvline(x=0, color="red");
```
<br/>

:::
::: {.column width="40%" }

> We will work more with `InferenceData` and `ArviZ` in the hands-on session.


- DISCUSS THE OUTPUT
:::
:::


## Value Conversion

<br>

::: columns
::: {.column width="60%" }
- So far we have compared A and B variants in terms of how many `subscription` they generate
- We estimated the relative uplift of conversions between A and B

> We solved a [Bernoulli Conversion]{style="background-color: yellow;"} problem
:::
::: {.column width="40%" }
![](img/AdobeStock_120197715.jpeg){width="75%" fig-align="left"}
:::
:::

<br>

::: columns
::: {.column width="60%" }
- Now what if we wanted to compare A and B variants in terms of how much `revenue` they generate
- We would like to estimate the relative `uplift of revenue` between A and B  

> We'd like to solve a [Value Conversion]{style="background-color: yellow;"} problem
::: 
::: {.column width="40%" }
![](img/AdobeStock_545777518.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::

## Setting up the Model

<br>

::: columns
::: {.column width="60%" }
Suppose that a single visitor $i$ has a probability $\theta$ of paying at all. So, we can model the conversion as a Bernoulli trial:

$$conv_i \sim \mathrm{Bernoulli}(\color{red} \theta)$$ 

::: fragment
$$\color{red} \theta \sim \mathrm{Beta}(\alpha_1, \beta_1)$$
:::
:::
::: {.column width="40%" }
![](img/AdobeStock_120197715.jpeg){width="75%" fig-align="left"}
:::
:::

<br>

::: columns
::: {.column width="60%" }
We assume that if a visitor pays, they spend an amount $rev$ that is exponentially distributed with rate $\lambda$:

$$rev_i \sim \mathrm{Exponential}(\color{red} \lambda)$$

::: fragment
$$\color{red} \lambda \sim \mathrm{Gamma}(\alpha_2, \beta_2)$$
:::
::: 
::: {.column width="40%" }
![](img/AdobeStock_545777518.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::
::: notes 
This is already the Bernoulli conversion model we have seen before. What is new in the Value Conversion model is the following.
In this setting, we have two priors, one for $\theta$ and one for $\lambda$. So, we need two likelihoods, one for $conv_i$ and one for $rev_i$. By combining the two priors and likelihoods, we can estimate two posteriors, one for $\theta$ and one for $\lambda$.
:::

## What Do We Need?

<br>
Our goal is to find out `revenue per visitor`. The expected revenue per visitor is $\frac{\theta}{\lambda}$. 

So we need:

- the posterior of $\theta$
- the posterior of $\lambda$
- to calculate $\frac{\theta}{\lambda}$
- to calculate the uplift of revenue between A and B

<br>

> We should go the same way as before:\ we need to define the `priors` and `likelihoods`, and obtain `posteriors` for $\theta$ and $\lambda$.

## What Do We Need?
#### Likelihoods

In reality we have more than just a single visitor.

<br>

So, the total conversions follows a Binomial distribution:

::: columns
::: {.column width="70%" }

\begin{align*}
conv &=  \sum^{\# \text{visitors}} conv_i \\
&= \sum^{\# \text{visitors}}\mathrm{Bernoulli}(\theta)
\sim \mathrm{Binomial}(\# \text{visitors}, \theta)
\end{align*}
:::
::: {.column width="30%" }
![](img/AdobeStock_607420431.png){width="25%" fig-align="left"}
:::
:::

<br>

The total revenue follows a Gamma distribution:

::: columns
::: {.column width="70%" }

\begin{align*}
rev &= \sum^{\# \text{payers}} rev_i \\
&= \sum^{\# \text{payers}}\mathrm{Exponential}(\lambda)
\sim \mathrm{Gamma}(\# \text{payers}, \lambda)
\end{align*}
:::
::: {.column width="30%" }
![](img/AdobeStock_607420431.png){width="25%" fig-align="left"}
:::
:::





::: notes
:::

## What Do We Need?
#### Priors and Likelihoods

Let's complete the setting by bringing in the variants A and B:

We have two [conversion likelihoods]{style="background-color: Lightyellow;"}:

\begin{align*}
conv_A &\sim \mathrm{Binomial}(N_A, \color{red}{\theta_A}) \\
conv_B &\sim \mathrm{Binomial}(N_B, \color{red}{\theta_B}) \\
\end{align*}

and our [prior]{style="background-color: rgba(255, 99, 71, 0.2);"} about conversion probabilities are:

$$\color{red}{\theta_A} = \color{red}{\theta_B} \sim \mathrm{Beta}(\alpha_1, \beta_1)$$

<br>

We have two [revenue likelihoods]{style="background-color: Lightyellow;"}:

\begin{align*}
rev_A &\sim \mathrm{Gamma}(conv_A, \color{red}{\lambda_A}) \\
rev_B &\sim \mathrm{Gamma}(conv_B, \color{red}{\lambda_B})
\end{align*}

and our [prior]{style="background-color: rgba(255, 99, 71, 0.2);"} about the amounts spent are:

$$\color{red}{\lambda_A} = \color{red}{\lambda_B} \sim \mathrm{Gamma}(\alpha_2, \beta_2)$$

::: notes
:::



<!-- ## Posteriors

<br>

We have four posteriors:

<br>

$$\theta: \left\{
    \begin{array}{ll}
    \theta_A \sim \mathrm{Beta}(\alpha_1 + conv_A, \beta_1 + N_A - conv_A) \\
    \theta_B \sim \mathrm{Beta}(\alpha_1 + conv_B, \beta_1 + N_B - conv_B) 
    \end{array}
\right.$$

<br>

$$\lambda: \left\{
    \begin{array}{ll}
    \lambda_A \sim \mathrm{Gamma}(\alpha_2 + conv_A, \beta_2 + rev_A) \\
    \lambda_B \sim \mathrm{Gamma}(\alpha_2 + conv_B, \beta_2 + rev_B) 
    \end{array}
\right.$$ -->

## Summary

<br>

![](img/frame.jpg){width="70%" height="70%" fig-align="left"}


::: columns
::: {.column width="30%" .fragment}
$$\theta: \left\{
    \begin{array}{ll}
    \theta_A  \sim \mathrm{Beta}(\alpha_1, \beta_1) \\
    \theta_B \sim \mathrm{Beta}(\alpha_1, \beta_1) 
    \end{array}
\right.$$
:::

::: {.column width="10%" .fragment}
![](img/AdobeStock_607420431.png){width="70%" height="70%" fig-align="right"}
:::

::: {.column width="60%" .fragment}
$$\begin{array}{ll}
    \theta_A \sim \mathrm{Beta}(\alpha_1 + conv_A, \beta_1 + N_A - conv_A) \\
    \theta_B \sim \mathrm{Beta}(\alpha_1 + conv_B, \beta_1 + N_B - conv_B) 
    \end{array}$$
:::

:::

<br>

::: columns
::: {.column width="30%" .fragment}
$$\lambda: \left\{
    \begin{array}{ll}
    \lambda_A \sim \mathrm{Gamma}(\alpha_2, \beta_2) \\
    \lambda_B \sim \mathrm{Gamma}(\alpha_2, \beta_2) 
    \end{array}
\right.$$
:::
::: {.column width="10%" .fragment}
![](img/AdobeStock_607420431.png){width="70%" height="70%" fig-align="right"}
:::
::: {.column width="60%" .fragment}
$$\begin{array}{ll}
    \lambda_A \sim \mathrm{Gamma}(\alpha_2 + conv_A, \beta_2 + rev_A) \\
    \lambda_B \sim \mathrm{Gamma}(\alpha_2 + conv_B, \beta_2 + rev_B) 
    \end{array}$$
:::
:::

## What Do We Want to Know?


Our goal is to find out which version is more profitable, so we should compare the `revenue per visitor` between A and B.

We obtain the `revenue per visitor` for each variant by:

$$\mu_A =  \dfrac{\theta_A}{\lambda_A}, \quad  \mu_B =  \dfrac{\theta_B}{\lambda_B}$$

and the relative uplift is:

$$uplift = \frac{\mu_B}{\mu_A} - 1$$

- if the `uplift > 0`, then version B is more profitable
- if the `uplift < 0`, then version A is more profitable
- if the `uplift = 0`, then both are equally profitable


::: fragment
::: callout-tip
Again, we get the whole distribution of the uplift, not just these three states ðŸ˜ƒ
:::
:::



::: notes
$\mu$ here represents the average revenue per visitor, including those who don't make a purchase. This is the best way to capture the overall revenue effect - some variants may increase the average sales value, but reduce the proportion of visitors that pay at all (e.g. if we promoted more expensive items on the landing page).
:::

<!-- ## Let's do this in ![](https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg){.center} -->
## Creating the Model in ![](img/PyMC.png){.center}

<!-- ```{python}
#| echo: false
#| eval: false
# as before we have variants A and B
variants  = ['A', 'B']
# let's define the revenue data
# each variant has 1000 visitors 
visitors      = [1000, 1000]

# 100 of which leads to purchase, i.e. conversion rate is 10%
purchased     = [100, 100]

# each purchase is worth 10, i.e. mean purchase is 10
total_revenue = [1000, 1000]

# let's define parameters for a prior for the conversion rates
conv_alpha, conv_beta = [5000, 5000]

# let's define parameters for the mean purchase prior
purchase_alpha, purchase_beta = [9000, 900]
``` -->


```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 1|3,17|3-15|5-8|11-15|17-29|19-22|25-29|32|32,37|35-36|41
with pm.Model() as example_model:

    #-----------------------------------------------conversion rate model
    # Priors for unknown model parameters
    theta = pm.Beta("theta",
                    alpha = conv_alpha, 
                    beta  = conv_beta, 
                    shape = 2)
    
    # Likelihood of observations
    converted = pm.Binomial("converted", 
                            n        = visitors,      
                            observed = purchased,     
                            p        = theta,         
                            shape    = 2)  
    
    #------------------------------------------------revenue model
    # Priors for unknown model parameters
    lamda = pm.Gamma( "lamda", 
                    alpha = purchase_alpha,
                    beta  = purchase_beta,
                    shape = 2)
    
    # Likelihood of observations
    revenue = pm.Gamma("revenue", 
                        alpha    = purchased,            
                        observed = total_revenue, 
                        beta     = lamda, 
                        shape    = 2)        
    
    # get the revenue per visitor
    revenue_per_visitor = pm.Deterministic("revenue_per_visitor", theta / lamda)

    #------------------------------------------------relative uplifts
    theta_uplift = pm.Deterministic(f"theta uplift", theta[1] / theta[0] - 1)
    lamda_uplift = pm.Deterministic(f"lamda uplift", (1 / lamda[1]) / (1 / lamda[0]) - 1)
    uplift       = pm.Deterministic(f"uplift", revenue_per_visitor[1] / revenue_per_visitor[0] - 1)

    #------------------------------------------------posterior
    # draw posterior samples
    trace = pm.sample(draws=10, return_inferencedata=True)
```


## {chalkboard-buttons="false" background-image="../assets/img/Handson_AdobeStock_316152114.jpeg" background-position="center"  background-repeat="no-repeat" background-opacity="0.5" background-size="cover"}

![](../assets/img/python-logo-inkscape.svg){width="30%" height="30%" fig-align="center"}

After the coffee break:

-   Open the Jupyter notebook in `Day2-02/hands-on.ipynb`
-   Follow the instructions in the notebook, and run the codes.
-   You will be able to find the solution to the exercise in the `Day2-02/hands-on-solution.ipynb` notebook by the end of the day.

::: footer
:::

## {chalkboard-buttons="false" background-image="../assets/img/Quiz_AdobeStock_439133763.jpeg" background-position="center"  background-repeat="no-repeat" background-opacity="0.9" background-size="cover"}

## {chalkboard-buttons="false" background-image="../assets/img/Coffee_AdobeStock_318442666.jpeg"  background-size="cover" background-opacity="0.8" }

::: footer

:::