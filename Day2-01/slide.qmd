---
title: Bayesian AB Testing
subtitle: Bernoulli and Value Conversions
author: EPFL Extension School
editor:
  render-on-save: true
format:
  revealjs:
    code-copy: true
    code-link: true
    incremental: true
    code-color-bg: "#2d2d2d"
    embed-resources: false
    smaller: true
    scrollable: true
    theme: simple
    logo: img/logo_red.png
    code-fold: true
    slide-number: true
    chalkboard:
      buttons: true
    preview-links: auto
    menu: false
from: markdown+emoji
execute:
  enabled: true
  cache: true
jupyter: python3
---

## Introduction

- A/B testing is a method of comparing two versions of a webpage or app against each other to determine which one performs better.
- AB testing is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal.


## Defining the problem

- We have a website and we want to increase the number of visitors that click on a button.
- We have two versions of the website: A and B.
- We want to know which version is better.
- We want to know how much better it is.
- We want to know how much data we need to collect to be sure that we have a good estimate of the difference between the two versions.
- We consider Bernoulli conversion

## Definitions
### Parameters

- $\theta_A$ is the conversion rate for version A
- $\theta_B$ is the conversion rate for version B
- $\frac{\theta_B}{\theta_A}-1$ is the uplift of version B compared to version A

### Priors

-  conversion rates $\theta$ are independent
-  conversion rates $\theta$ are Beta distributed
$$\theta_A \sim Beta(\alpha_A, \beta_A)$$
$$\theta_B \sim Beta(\alpha_B, \beta_B)$$
-  conversion rates share the same parameters $\alpha_A=\beta_A=\alpha_B=\beta_B=100$

## Definitions

### Likelihoods

-  data for version A and B are independent
-  data for version A and B are Bernoulli distributed

### Posterior

-  posterior is Beta distributed


## PyMC

We use PyMC library to do the Bayesian inference for the A/B testing problem.

TALK ABOUT PYMC HERE

```{python}
#| echo: false
#| eval: true

import matplotlib.pyplot as plt
import pandas as pd
import arviz
import numpy as np
import pymc as pm
rng = np.random.default_rng(4000)
__spec__ = None
plotting_defaults = dict(
    bins=50,
    kind="hist",
    textsize=8,
)
```

```{.python}
import pymc as pm
```

## Setting up the problem in PyMC


```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2|4,5|7,8|10,11

# let's define the variants A and B
variants  = ['A', 'B']

# each variant has 1000 trail 
trials    = [1000, 1000]

# 200 of which leads to success
successes = [200, 200]

# let's define parameters for a weak prior for the conversion rates
weak_alpha, weak_beta = [100, 100]
```

```{python}
#| echo: false
#| eval: true
#| code-fold: false
from scipy.stats import beta

# plot probability distribution
x = np.linspace(0,1,100)
fig, ax = plt.subplots(figsize=(5, 3))
plt.plot(x, beta.pdf(x, weak_alpha,weak_beta), 'r-', lw=2, )
plt.xlabel(r'$\theta$')
plt.title(r'prior $\beta(100,100)$ PDF')
plt.show()
```



## Creating a model in PyMC

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1|4|5,6|7|10|11,12|4,13|7,14|4,17,18|21
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n        = trials, 
                      observed = successes, 
                      p        = theta,
                      shape    = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the posterior
    trace = pm.sample(draws=1000, return_inferencedata=False, progressbar=False)
```


![](img/Screenshot%202023-07-16%20at%2016.12.14.png)

::: notes
how to explain the with statement in pymc?

the pymc code looks like this: it hijacks the context manager which one usually uses for 
opening/closing files and instead uses it to create a context for the model and define its variables. Here it opens a model (WITH pm.Model() AS model)
and populate it with the priors and variables. Then it runs the model and samples from the posterior. 
Model is an object which is a container  for the model random variables. It is the top level container for all probability models. 

Finally it returns the posterior samples. The context manager is a python construct that
allows you to do something before and after a block of code. In this case it opens the model
and closes it after the sampling is done.
Any time you declare a pymc mvariable inside a with statement (or context manager), it gets added to the model. Pymc is a high level language.
:::

## Checking the model specification

Unobserved Random Variables:

```{python}
#| code-block-bg: true
#| code-block-border-left: "#31BAE9"
example_model.unobserved_RVs  
```

<br>

Observed Random Variables:

```{python}
#| code-block-bg: true
#| code-block-border-left: "#31BAE9"
example_model.observed_RVs  
```

```{python}
#| code-block-bg: true
#| code-block-border-left: "#31BAE9"
#| echo: true
#| eval: true
#| code-fold: false

trace
```



<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.theta.shape,  trace.uplift.shape
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
# let's put the trace into a dataframe
weak_outcome = pd.concat([pd.DataFrame(trace['theta']), 
                          pd.DataFrame(trace['uplift'])],
                          axis=1)
weak_outcome.columns = ['theta_A','theta_B','uplift']
```


## Ploting the output

::: panel-tabset
### plot
```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 5,9,13
# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift
ax[2].hist(weak_outcome['uplift'], bins=50,)
ax[2].set_title('uplift')

plt.show()
```

### show it
```{python}
#| echo: false
#| eval: true


# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift
ax[2].hist(weak_outcome['uplift'], bins=50,)
ax[2].set_title(r'uplift: $\frac{\theta_B}{\theta_A}-1$')

plt.show()
```

:::

## Changing the output type

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 21,22
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n = trials, 
                      p = theta, 
                      observed = successes,
                      shape = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the prior
    trace = pm.sample(draws=1000, return_inferencedata=True, progressbar=False)

```

## Changing the output type


```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace
```

<br>

Note that the output type is now different:

- Each row of an array is treated as an independent series of draws from the variable, called a chain.

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.posterior.theta.shape
```

::: notes
as an example, for theta we have 4 chains, each with 1000 draws. So for theta_A we have 4 chains, each with 1000 draws.
:::


## Exploring the posterior


```{python}
#| echo: true
#| eval: true
#| code-fold: false

pm.summary(trace, var_names=["theta", "uplift"]).T
```

## Exploring the posterior

we can also use the ArviZ library for plotting. The main advantage is that it works pretty well with the output data. Also, it gives the HDI for the Relative Uplift distribution.

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2
arviz.plot_posterior(trace.posterior["uplift"], **plotting_defaults, figsize=(6, 4))

plt.title("B vs. A Relative Uplift Distribution", fontsize=10)
plt.axvline(x=0, color="red");
```

what is HDI?

## Value Conversion

- So far we have compared A and B variants in terms of how many conversions they generate, and estimated the relative uplift of conversions between the two.

- Now what if we wanted to compare A and B variants in terms of how much revenue they generate, and estimate the relative uplift of revenue between the two. 

- We should consider Value Conversion

## Definitions

Suppose that a single visitor $i$ has a probability $\theta$ of paying at all. So, we can model the conversion as a Bernoulli trial:

$$conv_i \sim \mathrm{Bernoulli}(\theta)$$ 

$$\theta \sim \mathrm{Beta}(\alpha_1, \beta_1)$$

We assume that if a visitor pays, they spend an amount $rev$ that is exponentially distributed with rate $\lambda$:

$$rev_i \sim \mathrm{Exponential}(\lambda)$$

$$\lambda \sim \mathrm{Gamma}(\alpha_2, \beta_2)$$

::: notes 
This is already the Bernoulli conversion model we have seen before. What is new in the Value Conversion model is the following.
In this setting, we have two priors, one for $\theta$ and one for $\lambda$. So, we need two likelihoods, one for $conv_i$ and one for $rev_i$. By combining the two priors and likelihoods, we can estimate two posteriors, one for $\theta$ and one for $\lambda$.
:::

## Objective

The reach our objective which is finding the relative uplift of revenue between A and B we will use the posterior samples of $\theta$ and $\lambda$. The expected revenue per visitor is $\theta/\lambda$.

What are the likelihoods for the Value Conversion model?


## Likelihoods

Let's extend the setting for more than just a single visitor:

\begin{align*}
conv &=  \sum^{\# \text{visitors}} conv_i \\
&= \sum^{\# \text{visitors}}\mathrm{Bernoulli}(\theta)
\sim \mathrm{Binomial}(\# \text{visitors}, \theta)
\end{align*}

\begin{align*}
rev &= \sum^{\# \text{payers}} rev_i \\
&= \sum^{\# \text{payers}}\mathrm{Exponential}(\lambda)
\sim \mathrm{Gamma}(\# \text{payers}, \lambda)
\end{align*}

::: notes
:::

## Summary

Let's complete the setting by bringing in the variants A and B:

We have two conversion **likelihoods**:

\begin{align*}
conv_A &\sim \mathrm{Binomial}(N_A, \theta_A) \\
conv_B &\sim \mathrm{Binomial}(N_B, \theta_B) \\
\end{align*}

and our **prior** about conversion probablies are:

$$\theta_A = \theta_B \sim \mathrm{Beta}(\alpha_1, \beta_1)$$

We have two revenue **likelihoods**:

\begin{align*}
rev_A &\sim \mathrm{Gamma}(conv_A, \lambda_A) \\
rev_B &\sim \mathrm{Gamma}(conv_B, \lambda_B)
\end{align*}

ad our **prior** about the amounts spent are:

$$\lambda_A = \lambda_B \sim \mathrm{Gamma}(\alpha_2, \beta_2)$$

::: notes
:::

## Uplift


Finally, average spending by two variants are:

$$\mu_A =  \dfrac{\theta_A}{\lambda_A}$$
$$\mu_B =  \dfrac{\theta_B}{\lambda_B}$$

and the relative uplift is:

$$\mathrm{uplift} = \mu_B / \mu_A - 1$$

::: notes
$\mu$ here represents the average revenue per visitor, including those who don't make a purchase. This is the best way to capture the overall revenue effect - some variants may increase the average sales value, but reduce the proportion of visitors that pay at all (e.g. if we promoted more expensive items on the landing page).
:::

## Posterior

We have two posteriors:

$$\theta_A, \theta_B \sim \mathrm{Beta}(\alpha_1 + conv_A, \beta_1 + N_A - conv_A)$$

$$\lambda_A, \lambda_B \sim \mathrm{Gamma}(\alpha_2 + rev_A, \beta_2 + conv_A)$$

## Creating this model in PyMC

<!-- ```{python}
#| echo: false
#| eval: false
# as before we have variants A and B
variants  = ['A', 'B']
# let's define the revenue data
# each variant has 1000 visitors 
visitors      = [1000, 1000]

# 100 of which leads to purchase, i.e. conversion rate is 10%
purchased     = [100, 100]

# each purchase is worth 10, i.e. mean purchase is 10
total_revenue = [1000, 1000]

# let's define parameters for a prior for the conversion rates
conv_alpha, conv_beta = [5000, 5000]

# let's define parameters for the mean purchase prior
purchase_alpha, purchase_beta = [9000, 900]
``` -->


```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 1|3,17|3-15|5-8|11-15|17-29|19-22|25-29|32|32,37|35-36
with pm.Model() as example_model:

    #-----------------------------------------------conversion rate model
    # Priors for unknown model parameters
    theta = pm.Beta("theta",
                    alpha = conv_alpha, 
                    beta  = conv_beta, 
                    shape = 2)
    
    # Likelihood of observations
    converted = pm.Binomial("converted", 
                            n        = visitors,      
                            observed = purchased,     
                            p        = theta,         
                            shape    = 2)  
    
    #------------------------------------------------revenue model
    # Priors for unknown model parameters
    lamda = pm.Gamma( "lamda", 
                    alpha = purchase_alpha,
                    beta  = purchase_beta,
                    shape = 2)
    
    # Likelihood of observations
    revenue = pm.Gamma("revenue", 
                        alpha    = purchased,            
                        observed = total_revenue, 
                        beta     = lamda, 
                        shape    = 2)        
    
    # get the revenue per visitor
    revenue_per_visitor = pm.Deterministic("revenue_per_visitor", theta / lamda)

    #------------------------------------------------relative uplifts
    theta_uplift = pm.Deterministic(f"theta uplift", theta[1] / theta[0] - 1)
    lamda_uplift = pm.Deterministic(f"lamda uplift", (1 / lamda[1]) / (1 / lamda[0]) - 1)
    uplift       = pm.Deterministic(f"uplift", revenue_per_visitor[1] / revenue_per_visitor[0] - 1)

    #------------------------------------------------posterior
    # draw posterior samples
    trace = pm.sample(draws=10, return_inferencedata=True)
```