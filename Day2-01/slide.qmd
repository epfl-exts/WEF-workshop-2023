---
title: Bayesian AB Testing
subtitle: Bernoulli and Value Conversions
author: EPFL Extension School
institute: World Economic Forum
date: 2023-09-29
editor:
  render-on-save: true
format:
  revealjs:
    code-copy: true
    code-link: true
    incremental: false
    embed-resources: false
    smaller: true
    scrollable: false
    theme: ../assets/theme.scss 
    # theme: simple
    logo: ../assets/img/logo_red.png
    code-fold: true
    code-line-numbers: false
    slide-number: true
    chalkboard:
      buttons: true
    preview-links: auto
    menu: false
from: markdown+emoji
execute:
  enabled: true
  cache: true
jupyter: python3
---

## {chalkboard-buttons="false" background-image="../assets/img/AdobeStock_536776996.jpeg"  background-size="cover" }

 
[Goal of This Session:]{style="font-size: 1.5em; font-weight: bold;"}

<br>


:heavy_check_mark: Python

:heavy_check_mark: Statistical Distributions

:heavy_check_mark: Bayesian Statistics

:heavy_check_mark: Bayesians vs Frequentists

:heavy_check_mark: AB Testing

&#9744; [AB Testing in Python]{style="color:#ff412c;"}


## Recall from Yesterday

- A/B testing is a method of comparing two versions of a webpage or app against each other to determine which one performs better.
- We have a website and we want to increase the number of visitors that click on a button.
- We have two versions of the website: A and B.
- We want to know which version is better.
- We want to know how much better it is.
- We want to know how much data we need to collect to be sure that we have a good estimate of the difference between the two versions.
- We consider Bernoulli conversion


## Setting up the Model

<br>


::: columns
::: {.column width="70%" }

#### Parameters

- $\theta_A$ is the conversion probability for version A
- $\theta_B$ is the conversion probability for version B
- $\frac{\theta_B}{\theta_A}-1$ is the uplift of version B compared to version A
:::
::: {.column width="30%" }
![](img/AdobeStock_248944797.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::

<br>

#### Priors
::: columns
::: {.column width="70%" }


-  Conversion rates $\theta$ are Beta distributed:
$$\theta_A \sim Beta(\alpha_A, \beta_A)$$
$$\theta_B \sim Beta(\alpha_B, \beta_B)$$
-  For simplicity, parameters and distributions are the same: $\alpha_A=\beta_A=\alpha_B=\beta_B$
::: 
::: {.column width="30%" }
![](img/AdobeStock_105107192.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::





## Setting up the Model

<br>



::: columns
::: {.column width="70%" }
#### Likelihoods

- Both versions have the same number of `visitors` 
- `conversions` are happenning independently
-  Number of `conversions` are Bernoulli distributed for both versions
:::
::: {.column width="30%" }
![](img/AdobeStock_124620223.jpeg){width="75%" height="65%" fig-align="left"}
:::
:::


<br>

#### Posteriors

::: columns
::: {.column width="70%" }
- Remember that in this case posteriors are also Beta distributed, thanks to the conjugate priors
- No worries if we don't have a closed form solution for the posterior, we get them via sampling methods
::: 
::: {.column width="30%" }
![](img/AdobeStock_109501465.jpeg){width="75%" height="75%" fig-align="left"}
:::
:::


::: notes
visitors are like the trials or coin flips in the Bernoulli distribution. We have 1000 trials for each variant.
conversions are like the successes or number of heads in the Bernoulli distribution. We have 200 successes for each variant.
:::


## What do we want to know?

<br>
Our goal is to find out which version is better, and that's why we need the uplift $\frac{\theta_B}{\theta_A}-1$:

- if the uplift > 0, then version B is better
- if the uplift < 0, then version A is better
- if the uplift = 0, then there is no difference between the two versions

::: box
Luckily we get the whole posterior distribution of the uplift, not just these three states
:::

this is where we need ![](https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg){width="150px" height="150px" fig-align="left"} 



**PyMC** is a python library for Bayesian statistical modeling. We use it to do the Bayesian inference for the A/B testing problem. 

<br>

```{.python}
import pymc as pm
```

```{python}
#| echo: false
#| eval: true

import matplotlib.pyplot as plt
import pandas as pd
import arviz
import numpy as np
import pymc as pm
rng = np.random.default_rng(4000)
__spec__ = None
plotting_defaults = dict(
    bins=50,
    kind="hist",
    textsize=8,
)
```



::: notes
is a python library for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems. Along with core sampling functionality, PyMC includes methods for summarizing output, plotting, goodness-of-fit and convergence diagnostics.
:::


## Define Parameters in ![](img/PyMC.png){.center}

```{python}


```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2|4,5|7,8|10,11

# let's define the variants A and B
variants  = ['A', 'B']

# each variant has 1000 trail 
visitors    = [1000, 1000]

# 200 of which leads to success
conversion = [200, 200]

# let's define parameters for a weak prior for the conversion rates
weak_alpha, weak_beta = [100, 100]
```

```{python}
#| echo: false
#| eval: true
#| code-fold: false
from scipy.stats import beta

# plot probability distribution
x = np.linspace(0,1,100)
fig, ax = plt.subplots(figsize=(5, 3))
plt.plot(x, beta.pdf(x, weak_alpha,weak_beta), 'r-', lw=2, )
plt.xlabel(r'$\theta$')
plt.title(r'prior $\beta(100,100)$ PDF')
plt.show()
```



## Creating a Model in ![](img/PyMC.png){.center}

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1|4|5,6|7|10|11,12|4,13|7,14|4,17,18|21
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n        = visitors, 
                      observed = conversion, 
                      p        = theta,
                      shape    = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the posterior
    trace = pm.sample(draws=1000, return_inferencedata=False, progressbar=False)
```


![](img/Screenshot%202023-07-16%20at%2016.12.14.png)

::: notes
how to explain the with statement in pymc?

the pymc code looks like this: it hijacks the context manager which one usually uses for 
opening/closing files and instead uses it to create a context for the model and define its variables. Here it opens a model (WITH pm.Model() AS model)
and populate it with the priors and variables. Then it runs the model and samples from the posterior. 
Model is an object which is a container  for the model random variables. It is the top level container for all probability models. 

Finally it returns the posterior samples. The context manager is a python construct that
allows you to do something before and after a block of code. In this case it opens the model
and closes it after the sampling is done.
Any time you declare a pymc mvariable inside a with statement (or context manager), it gets added to the model. Pymc is a high level language.
:::

## Checking the Model Specification

Unobserved Random Variables:

```{python}
#| echo: true
#| eval: true
#| code-fold: false
example_model.unobserved_RVs  
```

<br>

Observed Random Variables:

```{python}
#| echo: true
#| eval: true
#| code-fold: false
example_model.observed_RVs  
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false

trace
```



<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.theta.shape,  trace.uplift.shape
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
# let's put the trace into a dataframe
weak_outcome = pd.concat([pd.DataFrame(trace['theta']), 
                          pd.DataFrame(trace['uplift'])],
                          axis=1)
weak_outcome.columns = ['theta_A','theta_B','uplift']
```


## Ploting the Output

::: panel-tabset
### plot
```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 5,9,13
# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift
ax[2].hist(weak_outcome['uplift'], bins=50,)
ax[2].set_title('uplift')

plt.show()
```

### show it
```{python}
#| echo: false
#| eval: true


# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift
ax[2].hist(weak_outcome['uplift'], bins=50,)
ax[2].set_title(r'uplift: $\frac{\theta_B}{\theta_A}-1$')

plt.show()
```

:::

## Changing the Output Type

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 22,23
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n = visitors, 
                      p = theta, 
                      observed = conversion,
                      shape = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the prior
    trace = pm.sample(draws=1000, 
                      return_inferencedata=True)
```

## Changing the Output Type


```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace
```

<br>

Note that the output type is now different:

- Each row of an array is treated as an independent series of draws from the variable, called a chain.

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.posterior.theta.shape
```

::: notes
as an example, for theta we have 4 chains, each with 1000 draws. So for theta_A we have 4 chains, each with 1000 draws.
:::


## Exploring the Posterior


```{python}
#| echo: true
#| eval: true
#| code-fold: false

pm.summary(trace, var_names=["theta", "uplift"]).T
```

## Exploring the Posterior

we can also use the ArviZ library for plotting. The main advantage is that it works pretty well with the output data. Also, it gives the HDI for the Relative Uplift distribution.

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2
arviz.plot_posterior(trace.posterior["uplift"], **plotting_defaults, figsize=(6, 4))

plt.title("B vs. A Relative Uplift Distribution", fontsize=10)
plt.axvline(x=0, color="red");
```

what is HDI?

## Value Conversion

- So far we have compared A and B variants in terms of how many conversions they generate, and estimated the relative uplift of conversions between the two.

- Now what if we wanted to compare A and B variants in terms of how much revenue they generate, and estimate the relative uplift of revenue between the two. 

- We should consider Value Conversion

## Setting up the Model

Suppose that a single visitor $i$ has a probability $\theta$ of paying at all. So, we can model the conversion as a Bernoulli trial:

$$conv_i \sim \mathrm{Bernoulli}(\theta)$$ 

$$\theta \sim \mathrm{Beta}(\alpha_1, \beta_1)$$

We assume that if a visitor pays, they spend an amount $rev$ that is exponentially distributed with rate $\lambda$:

$$rev_i \sim \mathrm{Exponential}(\lambda)$$

$$\lambda \sim \mathrm{Gamma}(\alpha_2, \beta_2)$$

::: notes 
This is already the Bernoulli conversion model we have seen before. What is new in the Value Conversion model is the following.
In this setting, we have two priors, one for $\theta$ and one for $\lambda$. So, we need two likelihoods, one for $conv_i$ and one for $rev_i$. By combining the two priors and likelihoods, we can estimate two posteriors, one for $\theta$ and one for $\lambda$.
:::

## Objective

The reach our objective which is finding the relative uplift of revenue between A and B we will use the posterior samples of $\theta$ and $\lambda$. The expected revenue per visitor is $\theta/\lambda$.

What are the likelihoods for the Value Conversion model?


## Likelihoods

Let's extend the setting for more than just a single visitor:

\begin{align*}
conv &=  \sum^{\# \text{visitors}} conv_i \\
&= \sum^{\# \text{visitors}}\mathrm{Bernoulli}(\theta)
\sim \mathrm{Binomial}(\# \text{visitors}, \theta)
\end{align*}

\begin{align*}
rev &= \sum^{\# \text{payers}} rev_i \\
&= \sum^{\# \text{payers}}\mathrm{Exponential}(\lambda)
\sim \mathrm{Gamma}(\# \text{payers}, \lambda)
\end{align*}

::: notes
:::

## Priors and Likelihoods

Let's complete the setting by bringing in the variants A and B:

We have two conversion **likelihoods**:

\begin{align*}
conv_A &\sim \mathrm{Binomial}(N_A, \theta_A) \\
conv_B &\sim \mathrm{Binomial}(N_B, \theta_B) \\
\end{align*}

and our **prior** about conversion probablies are:

$$\theta_A = \theta_B \sim \mathrm{Beta}(\alpha_1, \beta_1)$$

We have two revenue **likelihoods**:

\begin{align*}
rev_A &\sim \mathrm{Gamma}(conv_A, \lambda_A) \\
rev_B &\sim \mathrm{Gamma}(conv_B, \lambda_B)
\end{align*}

ad our **prior** about the amounts spent are:

$$\lambda_A = \lambda_B \sim \mathrm{Gamma}(\alpha_2, \beta_2)$$

::: notes
:::

## Uplift


Finally, average spending by two variants are:

$$\mu_A =  \dfrac{\theta_A}{\lambda_A}$$
$$\mu_B =  \dfrac{\theta_B}{\lambda_B}$$

and the relative uplift is:

$$\mathrm{uplift} = \mu_B / \mu_A - 1$$

::: notes
$\mu$ here represents the average revenue per visitor, including those who don't make a purchase. This is the best way to capture the overall revenue effect - some variants may increase the average sales value, but reduce the proportion of visitors that pay at all (e.g. if we promoted more expensive items on the landing page).
:::

## Posterior

<br>

We have four posteriors:

<br>

$$\theta: \left\{
    \begin{array}{ll}
    \theta_A \sim \mathrm{Beta}(\alpha_1 + conv_A, \beta_1 + N_A - conv_A) \\
    \theta_B \sim \mathrm{Beta}(\alpha_1 + conv_B, \beta_1 + N_B - conv_B) 
    \end{array}
\right.$$

<br>

$$\lambda: \left\{
    \begin{array}{ll}
    \lambda_A \sim \mathrm{Gamma}(\alpha_2 + conv_A, \beta_2 + rev_A) \\
    \lambda_B \sim \mathrm{Gamma}(\alpha_2 + conv_B, \beta_2 + rev_B) 
    \end{array}
\right.$$

## Summary

<br>

![](img/frame.jpg){width="70%" height="70%" fig-align="left"}


::: columns
::: {.column width="30%" .fragment}
$$\theta: \left\{
    \begin{array}{ll}
    \theta_A  \sim \mathrm{Beta}(\alpha_1, \beta_1) \\
    \theta_B \sim \mathrm{Beta}(\alpha_1, \beta_1) 
    \end{array}
\right.$$
:::

::: {.column width="10%" .fragment}
![](img/AdobeStock_607420431.png){width="70%" height="70%" fig-align="right"}
:::

::: {.column width="60%" .fragment}
$$\begin{array}{ll}
    \theta_A \sim \mathrm{Beta}(\alpha_1 + conv_A, \beta_1 + N_A - conv_A) \\
    \theta_B \sim \mathrm{Beta}(\alpha_1 + conv_B, \beta_1 + N_B - conv_B) 
    \end{array}$$
:::

:::

<br>

::: columns
::: {.column width="30%" .fragment}
$$\lambda: \left\{
    \begin{array}{ll}
    \lambda_A \sim \mathrm{Gamma}(\alpha_2, \beta_2) \\
    \lambda_B \sim \mathrm{Gamma}(\alpha_2, \beta_2) 
    \end{array}
\right.$$
:::
::: {.column width="10%" .fragment}
![](img/AdobeStock_607420431.png){width="70%" height="70%" fig-align="right"}
:::
::: {.column width="60%" .fragment}
$$\begin{array}{ll}
    \lambda_A \sim \mathrm{Gamma}(\alpha_2 + conv_A, \beta_2 + rev_A) \\
    \lambda_B \sim \mathrm{Gamma}(\alpha_2 + conv_B, \beta_2 + rev_B) 
    \end{array}$$
:::
:::



<!-- ## Let's do this in ![](https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg){.center} -->
## Let's do this in ![](img/PyMC.png){.center}

<!-- ```{python}
#| echo: false
#| eval: false
# as before we have variants A and B
variants  = ['A', 'B']
# let's define the revenue data
# each variant has 1000 visitors 
visitors      = [1000, 1000]

# 100 of which leads to purchase, i.e. conversion rate is 10%
purchased     = [100, 100]

# each purchase is worth 10, i.e. mean purchase is 10
total_revenue = [1000, 1000]

# let's define parameters for a prior for the conversion rates
conv_alpha, conv_beta = [5000, 5000]

# let's define parameters for the mean purchase prior
purchase_alpha, purchase_beta = [9000, 900]
``` -->


```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 1|3,17|3-15|5-8|11-15|17-29|19-22|25-29|32|32,37|35-36|41
with pm.Model() as example_model:

    #-----------------------------------------------conversion rate model
    # Priors for unknown model parameters
    theta = pm.Beta("theta",
                    alpha = conv_alpha, 
                    beta  = conv_beta, 
                    shape = 2)
    
    # Likelihood of observations
    converted = pm.Binomial("converted", 
                            n        = visitors,      
                            observed = purchased,     
                            p        = theta,         
                            shape    = 2)  
    
    #------------------------------------------------revenue model
    # Priors for unknown model parameters
    lamda = pm.Gamma( "lamda", 
                    alpha = purchase_alpha,
                    beta  = purchase_beta,
                    shape = 2)
    
    # Likelihood of observations
    revenue = pm.Gamma("revenue", 
                        alpha    = purchased,            
                        observed = total_revenue, 
                        beta     = lamda, 
                        shape    = 2)        
    
    # get the revenue per visitor
    revenue_per_visitor = pm.Deterministic("revenue_per_visitor", theta / lamda)

    #------------------------------------------------relative uplifts
    theta_uplift = pm.Deterministic(f"theta uplift", theta[1] / theta[0] - 1)
    lamda_uplift = pm.Deterministic(f"lamda uplift", (1 / lamda[1]) / (1 / lamda[0]) - 1)
    uplift       = pm.Deterministic(f"uplift", revenue_per_visitor[1] / revenue_per_visitor[0] - 1)

    #------------------------------------------------posterior
    # draw posterior samples
    trace = pm.sample(draws=10, return_inferencedata=True)
```


## {chalkboard-buttons="false" background-image="../assets/img/Handson_AdobeStock_316152114.jpeg" background-position="center"  background-repeat="no-repeat" background-opacity="0.5" background-size="cover"}

![](../assets/img/python-logo-inkscape.svg){width="30%" height="30%" fig-align="center"}

After the coffee break:

-   Open the Jupyter notebook in `Day2-02/hands-on.ipynb`
-   Follow the instructions in the notebook, and run the codes.
-   You will be able to find the solution to the exercise in the `Day2-02/hands-on-solution.ipynb` notebook by the end of the day.

::: footer
:::

## {chalkboard-buttons="false" background-image="../assets/img/Coffee_AdobeStock_318442666.jpeg"  background-size="cover"}

::: footer

:::