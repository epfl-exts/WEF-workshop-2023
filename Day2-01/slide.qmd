---
title: Bayesian AB Testing
subtitle: Bernoulli and Value Conversions
author: EPFL Extension School
editor:
  render-on-save: false
format:
  revealjs:
    code-copy: true
    code-link: true
    incremental: true
    code-color-bg: "#2d2d2d"
    embed-resources: false
    smaller: true
    scrollable: true
    theme: simple
    logo: img/logo_red.png
    code-fold: true
    slide-number: true
    chalkboard:
      buttons: true
    preview-links: auto
    menu: false
from: markdown+emoji
execute:
  enabled: true
  cache: true
jupyter: python3
---


## Defining the problem

- We have a website and we want to increase the number of visitors that click on a button.
- We have two versions of the website: A and B.
- We want to know which version is better.
- We want to know how much better it is.
- We want to know how much data we need to collect to be sure that we have a good estimate of the difference between the two versions.
- We consider Bernoulli conversion

## Definitions
### Parameters

- $\theta_A$ is the conversion rate for version A
- $\theta_B$ is the conversion rate for version B
- $\frac{\theta_B}{\theta_A}-1$ is the uplift of version B compared to version A

### Priors

-  conversion rates $\theta$ are independent
-  conversion rates $\theta$ are Beta distributed

$$\theta_A \sim Beta(\alpha_A, \beta_A)$$
$$\theta_B \sim Beta(\alpha_B, \beta_B)$$

-  conversion rates share the same parameters $\alpha_A=\beta_A=\alpha_B=\beta_B$
-  $\alpha = \beta = 100$ 

## Definitions

### Likelihoods

-  data for version A and B are independent
-  data for version A and B are Bernoulli distributed

### Posterior

-  posterior is Beta distributed


## PyMC

We use PyMC library to do the Bayesian inference for the A/B testing problem.

TALK ABOUT PYMC HERE

```{python}
#| echo: false
#| eval: true

import matplotlib.pyplot as plt
import pandas as pd
import arviz as az
import numpy as np
import pymc as pm
rng = np.random.default_rng(4000)
__spec__ = None
plotting_defaults = dict(
    bins=50,
    kind="hist",
    textsize=8,
)
```

```{.python}
import pymc as pm
```

## Setting up the problem in PyMC


```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2|4,5|7,8|10,11

# let's define the variants A and B
variants  = ['A', 'B']

# each variant has 1000 trail 
trials    = [1000, 1000]

# 200 of which leads to success
successes = [200, 200]

# let's define parameters for a weak prior for the conversion rates
weak_alpha, weak_beta = [100, 100]
```


## Creating a model in PyMC

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1|4|5,6|7|10|11,12|4,13|14|4,17,18|21
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n        = trials, 
                      observed = successes, 
                      p        = theta,
                      shape    = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift_B", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the prior
    trace = pm.sample(draws=1000, return_inferencedata=False)
```

::: notes
how to explain the with statement in pymc?

the pymc code looks like this: it hijacks the context manager which one usually uses for 
opening/closing files and instead uses it to create a context for the model and define its variables. Here it opens a model (WITH pm.Model() AS model)
and populate it with the priors and variables. Then it runs the model and samples from the posterior. 
Model is an object which is a container  for the model random variables. It is the top level container for all probability models. 

Finally it returns the posterior samples. The context manager is a python construct that
allows you to do something before and after a block of code. In this case it opens the model
and closes it after the sampling is done.
Any time you declare a pymc mvariable inside a with statement (or context manager), it gets added to the model. Pymc is a high level language.
:::

## Checking the model specification

Unobserved Random Variables:

```{python}
#| code-block-bg: true
#| code-block-border-left: "#31BAE9"
example_model.unobserved_RVs  
```

<br>

Observed Random Variables:

```{python}
#| code-block-bg: true
#| code-block-border-left: "#31BAE9"
example_model.observed_RVs  
```

## Checking the output

```{python}
#| code-block-bg: true
#| code-block-border-left: "#31BAE9"
#| echo: true
#| eval: true
#| code-fold: false

trace
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace.theta.shape,  trace.uplift_B.shape
```

<br>

```{python}
#| echo: true
#| eval: true
#| code-fold: false
# let's create a dataframe with the output of the prior predictive
weak_outcome = pd.concat([pd.DataFrame(trace['theta']), 
                          pd.DataFrame(trace['uplift_B'])],
                          axis=1)
weak_outcome.columns = ['theta_A','theta_B','uplift_B']
```


## Ploting the output

::: panel-tabset
### plot
```{python}
#| echo: true
#| eval: false
#| code-fold: false
#| code-line-numbers: 5,9,13
# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift_B
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift_B
ax[2].hist(weak_outcome['uplift_B'], bins=50,)
ax[2].set_title('uplift_B')

plt.show()
```

### show it
```{python}
#| echo: false
#| eval: true


# let's create a  subplot with three plots in a row that shows the theta_A, theta_B and uplift_B
fig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)

# plot the distribution of theta_A
ax[0].hist(weak_outcome['theta_A'],bins=50,)
ax[0].set_title(r'$\theta_A$')

# plot the distribution of theta_B
ax[1].hist(weak_outcome['theta_B'],bins=50,)
ax[1].set_title(r'$\theta_B$')

# plot the distribution of uplift_B
ax[2].hist(weak_outcome['uplift_B'], bins=50,)
ax[2].set_title('uplift_B')

plt.show()
```

:::

## Changing the output type

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 21,22
with pm.Model() as example_model:

    # Priors for unknown model parameters
    theta = pm.Beta("theta", 
                    alpha = weak_alpha, 
                    beta  = weak_beta, 
                    shape = 2)
    
    # Likelihood (sampling distribution) of observations
    obs = pm.Binomial("y", 
                      n = trials, 
                      p = theta, 
                      observed = successes,
                      shape = 2) 
    
    # Difference between variants
    relative_uplift = pm.Deterministic("uplift_B", 
                                        theta[1] / theta[0] - 1)

    # Draw samples from the prior
    trace = pm.sample(draws=1000, return_inferencedata=True)

```

## Changing the output type


```{python}
#| echo: true
#| eval: true
#| code-fold: false
trace
```

<br>


Note that the output type is now different:

- Each row of an array is treated as an independent series of draws from the variable, called a chain.

## Exploring the posterior


```{python}
#| echo: true
#| eval: true
#| code-fold: false

pm.summary(trace, var_names=["theta", "uplift_B"]).T
```

## Exploring the posterior

we can also use the ArviZ library for plotting. The main advantage is that it works pretty well with the output data. Also, it gives the HDI for the Relative Uplift distribution.

```{python}
#| echo: true
#| eval: true
#| code-fold: false
#| code-line-numbers: 1,2
az.plot_posterior(trace.posterior["uplift_B"], **plotting_defaults, figsize=(6, 4))

plt.title("B vs. A Relative Uplift Distribution", fontsize=10)
plt.axvline(x=0, color="red");
```

what is HDI?