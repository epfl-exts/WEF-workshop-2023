---
title: Bayesian Statistics
subtitle: Bayes rule and Bayesian Inference
author: EPFL Extension School
editor:
  render-on-save: true
format:
  revealjs:
    embed-resources: false
    smaller: true
    scrollable: true
    theme: simple
    logo: img/logo_red.png
    footer: '<https://www.extensionschool.ch>'
    code-fold: true
    slide-number: true
    chalkboard:
      buttons: true
    preview-links: auto
    menu: false
    css: exts_theme.css
    mermaid:
      theme: forest
from: markdown+emoji
filters:
  - shinylive
execute:
  enabled: true
  cache: true
jupyter: python3

---

## Bayes rule 

<br> <br>

::: columns
::: {.column width="50%"}
::: fragment
![](img/clock.png){width="30%" height="20%" fig-align="center"}
:::
:::

::: {.column width="50%"}
::: incremental
-   `P(arrive on time) = 0.9`

-   `P(late) = 0.1`
:::
:::
:::

. . .

::: columns
::: {.column width="50%"}
-   That's our perception about how likely these two events are

-   Let's call that our [prior]{style="color:#2cd5ff"}
:::

::: {.column width="50%"}
![](img/thinking.png){width="30%" height="30%" fig-align="center"}
:::
:::

::: notes
Suppose that the probability that one arrives on time to work is 90%. The only other outcome is that they arrive late These two probabilities show how we perceive the two events and their associated possibilities, prior to gaining further knowledge about them. .nobullets ul {list-style-type: none;} <a href="https://www.flaticon.com/free-icons/late" title="Late icons">Late icons created by surang - Flaticon</a>
:::

## Bayes rule

<br> <br>

**But ...**

<br>

::: columns
::: {.column width="50%"}
::: incremental
-   What if we start learning more about the 'situation'?
-   Should we stick to our prior idea about the events and their possibilities?
-   If not, how they should be adapted to the new situation?
:::
:::

::: {.column width="50%"}
![](img/thinking.png){width="40%" height="30%" right="180" fig-align="center"}
:::
:::

. . .

This is where **Bayesian Statistics** steps in and provide answers

## Bayes rule {.smaller}

<br> Byes rule provides a simple passage from our **prior** idea to **new insight**

::: fragment
![](img/bayes%20-%20rule.png){width="70%" height="70%" fig-align="center"}
:::

::: fragment
![](img/bayes%20-%20rule%20-%20example.jpg){width="70%" height="70%" fig-align="center"}
:::

. . .

let's call the new idea a [posterior]{style="background-color: yellow;"}

. . .

`P(arrive on time) → P(arrive on time | it’s raining)`

::: notes
press c to activate the chalk and circle on the P(arrive on time \| it's raining) and the word posterior press c again to turn it off
:::

## Bayes rule

<br> <br> More formally, Bayes rule says:

$$P(\text{arrive on time} | rain) = \frac{P(rain | \text{arrive on time})P(\text{arrive on time})}{P(rain)}$$

## Bayes rule: an example

::: incremental
-   **Evidence**: a speaker at the forum will be from WEF NY office

-   **My prior**: 90% of WEF employees are on time

-   What is my **posterior**? what is the probability that the speaker will be on time?
:::

::: fragment
![](img/bayes-example.jpg){width="70%" height="70%" fig-align="center"}
:::

::: notes
posterior p(speaker is on time \| ny office) or posterior p(wef on time \| ny office)?
:::

## Bayes rule: an example {.smaller}

![](img/bayes-example.jpg){width="60%" height="60%" fig-align="center"}

::: {style="font-size:14px"}
::: fragment
```{=tex}
\begin{align}
P(\text{on time}|\text{NY office})& = \frac{P(\text{NY office}|\text{on time})\times P(\text{on time})}{P(\text{NY office})} \\
& = \frac{0.4 \times 0.9}{0.4 \times 0.9 + 0.8 \times 0.1} = 0.82
\end{align}
```
:::
:::

. . .

my prior has dropped from **90% to 82%**


::: notes
press c to activate the chalk and circle on the prior 90% and posterior 82% press c again to turn it off
:::

## How to obtain the posterior?

In Bayesian inference, we are interested in the posterior **distribution** of the parameters of interest, and not only in the **point** estimates.
So, question is how to obtain the posterior distribution? thinking face

There are at least two approaches:

- Analytical: based on exact formulas
- Numerical: based on sampling methods



## Analytical approach

Consider the example of flipping a coin, but other real-world examples include:

- the probability of a person having a disease
- the probability of a person buying a product
- the probability of a person clicking on an ad
- the probability of a person defaulting on a loan
- the probability of a person being left handed
- the probability of successful free throw by a player in basketball
- the probability that a baby is a girl
- the probability that a heart surgery patient will survive more than a year after surgery
- the probability that a widget on an assembly line is faulty

## Analytical approach

Here is what we want to do: [violet]{style="color:#962cff"} <-- [blue]{style="color:blue"} + [red]{style="color:red"} 

- we begin with some [prior]{style="color:blue"} distribution of the outcome of coin flips
- we observe some [data]{style="color:red"} that consist of a set of results from flipping the coin
- we infer the [posterior]{style="color:#962cff"} distribution of credibility using Bayes’ rule

$$\color{#962cff}{posterior} : \frac{\color{red}{likelihood}\times\color{blue}{prior}}{\color{red}{evidence}}$$

Mathematically:

$$\color{#962cff}{P(\theta|data)} = \frac{\color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}}{\color{red}{P(data)}}$$

or simply, $\color{#962cff}{P(\theta|data)} \propto \color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}$

or simply, $\color{#962cff}{posterior} \propto \color{red}{likelihood}\times\color{blue}{prior}$

::: notes
The “likelihood” p(D|θ), is the probability that the data could be generated by the model with parameter value θ.
The “evidence” for the model, p(D), is the overall probability of the data according to the model, averaged over all possible parameter values. It is the normalization constant that ensures that the posterior is a proper probability distribution.
The term “evidence” is in fact the “marginal likelihood” that refers specifically to the operation of taking the average of the likelihood, p(D|θ), across all values of θ, weighted by the prior probability of θ.
:::

## Likelihood ... but what is the likelihood? {.smaller}

let's go back to the coin example:

the probability of each outcome can be calculated directly from the Bernoulli distribution which has the form:

$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

where $x$ is the outcome of the coin flip (0 or 1) and $\theta$ is the probability of getting a head, and can be fixed to be 0.5, for example.

So, $P(x|\theta=0.5)$ is only a function of $x$.

## Likelihood  ... but what if we fix $x$ and let $\theta$ vary? {.smaller}

In this case we get a function of $\theta$ that is called the (Bernoulli) likelihood function for $\theta$:

$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

if we flip the coin N times and get $z=\sum_{i}x_i$  heads, then the likelihood function is:

$$P(x|\theta) = \theta^z(1-\theta)^{N-z}$$

Now we only need to specify the prior: 

$$\color{#962cff}{posterior} \propto \color{red}{likelihood}\times\color{blue}{prior}$$

## Prior  ... what should be the prior? {.smaller}

The prior $P(\theta)$ can be any function that gives probability for the values of $0 \leq \theta \leq 1$.

For example, we can choose a uniform prior, which means that all values of $\theta$ are equally likely. There are many other choices.

In Statistics, there are many instance where the choice is simply a matter of convenience. That's the case here for the prior:

The prior $P(\theta)$ ~ Beta distribution is a convenient choice because it is a **conjugate prior** to the Bernoulli likelihood function.

Conjugate prior means that the posterior distribution is of the same form as the prior distribution:

[prior]{style="color:red"} ~ Beta distribution $\rightarrow$ [posterior]{style="color:#962cff"} ~ Beta distribution

## Beta distribution

Let's see what the Beta distribution looks like:

$$P(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$

where $B(\alpha,\beta)$ is the beta function:

$$B(\alpha,\beta) = \int_0^1 \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta$$

and $\alpha>0$ and $\beta>0$ are the shape parameters of the distribution, and $0 \leq \theta \leq 1$.

## Beta distribution

Let's have a little fun with the [Beta distribution](https://shinylive.io/py/app/#code=NobwRAdghgtgpmAXGKAHVA6VBPMAaMAYwHsIAXOcpMAHQgEsZViAnMgAgGcALeibTgHcoZQtzqNmbdjBGoANsTLz6AIyzYFS9lE7sFZCU1YcIAVybYdeiKjoAzFsRhde-dpJPsAgujzsWSgATOBZ-M3oHJxdOQnocDE4yET1PaVU4ZLo6NFQAfQj2AF52CKwoAHM4PPt5CKCACjp2Ftc+AWFRbgwybjh4RIBrTLFsBoBKPGbWsvkobGIzMjzOehDVKBYmiFbd0vpyiDh5FbW4Da3pvev2AGJ2ABV6ZTgrm5ay7gAWJrAACWIgnYQWIcD0qE2sEyoT0xHs7F6cHYACFMlBgfQkiw1Et6KR2GIoBAqh4yHoeGg4AB+GhgSZvd73PioJZcFQhFgMm5lZlLFbs0K-KC0-y0gAKkPgFBYOhF7AAjP55QAGPDy+k7d4zA685acAWXMCqOXiyXQmXG-AKpWq9VTTVa-ZYIXsAB87FV7FUbo9tI1XJaGveZQhRxOsj420dHwOizILOWWjIv2g8DyqCC9j99veQdaGvG2QgdBC8M4oQAboLdf44wn-OXOKtSONEG8AAJ1pYdwIQDlYRTJqDyMhFWnedhigAiADE-W9S+xU9UM-YJm2HXtbBgWESQTBEnA4I15QBOABsAA5ldf1QG7gAC8scREBIkk5UAHiKAA9v-L7x-Yol0wFQIE4CFCDgBplQwZVFVg09TzwABWZVC03XYDAHJQGh-fwMmSLBMzw-xdQwKAJjwcjVAmSZ2AAckGABaBi8HkQQigAJgw95sLIZ55GghjUWSfQnA2VR6BUMgrBCcDnisewzAgQgBNIBjeJubCfzmDJ5AaFgGIAEhoGhEWSYzNPvbDsD045DJMsUmnMvpLPGazMNabChKqPsGkUQgigYjIkjY9hHChUgihnYdywwmyR0SbhAQmIscnQYDfFQBpcgKegG0rUJ-HWMwKiKB4WDMOBxjAABfPBwGXahAgARwiQJ4HITgeh-Mh8CIUgKCoZAeHaIQRDEeqAF0gA):

A few observations:

::: incremental
- uniform distribution is a special case of the Beta distribution e.g. with $\alpha=\beta=1$
- it can give symmetric distribution e.g. with $\alpha=\beta=4$
- it can give right or left-skewed distributions e.g. with $\alpha=2$ and $\beta=5$
:::

::: notes
note that changing the values of $\alpha$ and $\beta$ changes the shape of the distribution
:::


## Posterior  ... what would it be like if the prior is Beta? {.smaller}

![](img/posterior.png){width="45%" height="45%" fig-align="center"}

$$\color{#962cff}{P(\theta|data)} \propto \color{red}{\theta^z(1-\theta)^{N-z}}\times\color{blue}{\theta^{\alpha-1}(1-\theta)^{\beta-1}}$$

$$\color{#962cff}{P(\theta|data)} \propto \theta^{z+\alpha-1}(1-\theta)^{N-z+\beta-1}$$

$$\color{#962cff}{Beta(\alpha+z,\beta+N-z)} \propto \color{red}{\text{Bernoulli likelihood}} \times\color{blue}{Beta(\alpha,\beta)}$$



Let's have a little [show](https://shinylive.io/py/app/#code=NobwRAdghgtgpmAXGKAHVA6VBPMAaMAYwHsIAXOcpMAHQgEsZViAnMgAgGcALeibTgHcoZQtzqNmbdjBGoANsTLz6AIyzYFS9lE7sFZCU1YcIAVybYdeiKjoAzFsRhde-dpJPsAgujzsWSgATOBZ-M3oHJxdOQnocDE4yET1PaVU4ZLo6NFQAfQj2AF52CKwoAHM4PPt5CKCACjp2Ftc+AWFRbgwybjh4RIBrTLFsBoBKPGbWsvkobGIzMjzOehDVKBYmiFbd0vpyiDh5FbW4Da3pvev2AGJ2ABV6ZTgrm5ay7gAWJrAACWIgnYQWIcD0vTg7AAQpkoPpiElQvRWOwxFAIFU9KorBD9CxkSwdBAgtDQhBFvIVOwVMMVNxiMQggB+GhgSZvd73PioJZcFQhFgcm5lblLFb80K-KCs-ysgAKm1gmVCOhl7AAjP51QAGPDq9k7d4zA6i5acCWXMCqNXyxXwCiE634DVa3X6qaGo37DCm8VnS0AORtkAsGUJxHs7Fq8U4ap1rrd2oNXo+JtsYvN-t+EDyfSgQVjztZ5hgYfYEfYeYLat1Gu1ieTKbKqF+rINQpajeuzfRxzysj42xT3sWZB5yy0ZGzSryqCC9jbHveXfYBvG2QgdBCkc4oQAbpLTf5R+P-LvOKtSONEG8AAInpZ3wLE0JYRRTqDyMhFVnedhygARAAxNs3m3dhoHgWd5wmG9PT2WwMBYdEQRgRI4DgRp1QATgANgADm1Qj9Q7dhSNuAAC3cOFxZCMUhbUAB4ilQZj1VIiFkmKCDMBUCBOFQKBCDgBptQwbVNTE7DsL1et103eDdnuQhAhESEyEEYh9HfVIdjhJxBFI+x6AqfwoAADzBbiDESMxVEnTgGk1dgACZ-GMipVgALzgIonLcr5xnkzltO0GEWHJMxKXoal6Fpeh6UZUiLLBYBtQAXTfJQGk4qB-FygAqArTQwHMq0c8Z2AK9gnIAWly8YioaEqAwmWqSrKuB8wqyZ2AAchYPr-BgTgihc4KbmSyzODSzLqLycy5gyeQGkGgASGgaFyta+om64UpmjL0OWbAluOBo+ppY4EoZIJdqm1KjvmshnnkES+vCyLoti+LEruva9luA7ZowCp8UaAHdnI0KOFQJwNlUegVDIKwgnoJJ8VUJZkQU95gfVTLJxyvpkn8DJkiwGDcv8EqoAmPAStUCZer61RaqG6koGWoo+rhgl-2Aob5EEMbIdafHCffYnYTJ2FKfsaXSY8dMyAwOnxgAag63MuoLenlfHDAmc1lq2u18rmf8Vm6jgPdkTesgOeFsazPkVBuCgIoxNw-wzvkHnmERfEUUAoD7sU1oHpmgnjoWv3Vr6jatpJqAdrFloJdj06ufOvrw7x6bgBj57XvevmURQ+Eg-50POHzm5M7eqpiQaRRCB5jIkg5xwlVIIogM-Xd07uTOwbWCZSNImyXoqbhljmBYlgniOWhsnhAWXjdcm43wW1yAp6DPfdQn8dYzAqIoHhYMw4HGMAAF88HASCEGQQIAEcIkCeByE4HpzLIPgIgpAKBUGQDwdoQgRBiAfulIAA) of this










## Hands-on Time!

![](img/ai.ai){width="50%" height="50%" fig-align="center"}

![](img/python-logo-inkscape.svg){width="50%" height="50%" fig-align="center"}

It's your turn:

-   Open the notebook in `Day1-03/hands-on.ipynb`
-   Follow the instructions in the notebook
prior: how does an uninformative prior contribute to the posterior
sample size: how does contribute to the posterior

## Jupyter Notebook

```{python}
#| code-line-numbers: 7|12
#| eval: true
#| echo: true
#| output: true
# set the boundaries (-inf < a < b < inf)

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

a0, b0 = 1, 1

# plot probability distribution
x = np.linspace(0,1,100)

plt.plot(x, beta.pdf(x, a0,b0), 'r-', label='prior beta pdf',lw=3)
plt.xlabel('probability')
plt.ylabel('')
plt.title('')
plt.legend(loc='best', frameon=False)

plt.show()
```










##  {#slide1-id data-menu-title="coffee break"}

::: {style="font-size: 5em; font-color: #32a6a8 ;text-align: center"}
Coffee Break <br> :coffee:
:::

##  {background-image="img/working-hours-and-breaks-rules-made-simple-for-employers-and-employees-in-UK-by-Papershift.png" background-size="50%"}

::: footer
[image source](https://www.papershift.com/en/blog/working-hours-break-laws-in-uk-every-owner-employee-should-be-aware-of)
:::

##  {#slide3-id data-menu-title="QA"}

![](img/qa.png){width="30%" height="30%" fig-align="center"}

::: notes
<a href="https://www.flaticon.com/free-icons/qa" title="qa icons">Qa icons created by Freepik - Flaticon</a>
:::
