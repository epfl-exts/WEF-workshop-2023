---
title: Bayesian Statistics
subtitle: Bayes Rule and Bayesian Inference
author: EPFL Extension School
institute: World Economic Forum
date: 2023-09-28
editor:
  render-on-save: true
format:
  revealjs:
    code-copy: true
    code-link: true
    embed-resources: false
    smaller: true
    scrollable: false
    theme: ../assets/theme.scss 
    logo: ../assets/img/logo_red.png
    code-fold: true
    code-line-numbers: false
    slide-number: true
    chalkboard:
      buttons: true
    preview-links: auto
    menu: false
title-slide-attributes:
  data-background-image: img/AdobeStock_440229000.jpeg
  data-background-size: cover
  data-background-position: 100%, center
  data-background-opacity: ".15" 
from: markdown+emoji
filters:
  - shinylive
execute:
  enabled: true
  cache: true
jupyter: python3
---

## What's the Plan?
#### This session

<!---
 https://mermaid.js.org/syntax/flowchart.html 
--->

```{mermaid}

flowchart BT
    A([Python]) -----> E([PyMC])
    B([Statistics])--->D([A/B testing]) 
    B([Statistics]) ---> C([Bayes Rule]) 
    C([Bayes Rule]) --> D([A/B testing])  
    C([Bayes Rule]) ---> F[(Bayesian A/B Testing in PyMC)]
    E([PyMC]) --> F[(Bayesian A/B Testing in PyMC)]
    D([A/B testing])  ---> F[(Bayesian A/B Testing in PyMC)]
    click A "https://github.com/epfl-exts/WEF-workshop-2023/tree/main/Day1-01"
    click B "https://github.com/epfl-exts/WEF-workshop-2023/tree/main/Day1-02"
    click C "https://github.com/epfl-exts/WEF-workshop-2023/tree/main/Day1-03"
    click D "https://github.com/epfl-exts/WEF-workshop-2023/tree/main/Day1-04"
    click E "https://github.com/epfl-exts/WEF-workshop-2023/tree/main/Day2-01"
    click F "https://github.com/epfl-exts/WEF-workshop-2023/tree/main/Day2-01"
    linkStyle 3,4 stroke:#bcf5bc,stroke-width:6px
%%    style A fill:#bcf5bc
%%    style B fill:#bcf5bc
    style C fill:#bcf5bc
%%    style D fill:#bcf5bc
```

<!---
::: {.absolute top="58%" bottom="15%"  left="35%" right="80%" width="310" }
::: {.blackbox}
:::
:::
--->

## {chalkboard-buttons="false" background-image="../assets/img/AdobeStock_536776996.jpeg"  background-size="cover"}

 
[Goal of This Session:]{style="font-size: 1.5em; font-weight: bold;"}

<br>


:heavy_check_mark: Python

<br>

:heavy_check_mark: Statistical Distributions & Inference

<br>

&#9744; [Bayesian Statistics]{style="color:#ff412c;"}


## Bayes rule 

<br> <br>

::: columns
::: {.column width="50%"}
::: fragment
![](img/AdobeStock_228399688.jpeg){width="40%" height="40%" fig-align="center"}
:::
:::

::: {.column width="50%"}
::: incremental
-   `P(arrive on time) = 0.9`

<br>

-   `P(late) = 0.1`
:::
:::
:::

. . .

::: columns
::: {.column width="50%"}
-   That's our perception about how likely these two events are

-   Let's call that our [prior]{style="color: blue"} knowledge about the events
:::

::: {.column width="50%"}
![](img/AdobeStock_607202393.jpeg){width="50%" height="50%" fig-align="center"}
:::
:::

::: notes
Suppose that the probability that one arrives on time to work is 90%. The only other outcome is that they arrive late These two probabilities show how we perceive the two events and their associated possibilities, prior to gaining further knowledge about them.
:::

## Bayes rule

<br> <br>

**But ...** 



::: columns
::: {.column width="50%"}
-   What if we start learning more about the 'situation'?
-   Should we stick to our [prior]{style="color: blue"} idea about the events and their possibilities?
:::

::: {.column width="50%"}
![](img/AdobeStock_462364017.jpeg){width="50%" height="50%" right="180" fig-align="center"}
:::
:::

. . .

If not, how we should adapt our priors to the new situation?

. . .


<!-- ::: goal -->
> This is where Bayesian Statistics steps in and provide answers
<!-- ::: -->



## Bayes rule {.smaller}

<br> Bayes rule provides a simple passage from our [prior]{style="color: blue"} idea to [new insight]{style="background-color: #fadf43;"}

::: fragment
![](img/bayes-rule.jpeg){width="70%" height="70%" fig-align="center"}
:::

::: fragment
![](img/bayes-rule-example.jpg){width="70%" height="70%" fig-align="center"}
:::

. . .

Let's call the new idea a [posterior]{style="background-color: #fadf43;"}

. . .

`P(arrive on time)` → `P(arrive on time | it’s raining)` 

::: notes
press c to activate the chalk and circle on the P(arrive on time \| it's raining) and the word posterior press c again to turn it off
:::

## Bayes rule

<br> <br>

More formally, Bayes rule says:

<div style="position:relative; width:640px; height:480px; margin:0 auto;">
  <img class="fragment" src="img/AdobeStock_628760640.jpeg"   style="position:absolute;top:0;left:0;" />
  <!-- <img class="fragment" src="img/AdobeStock_628760640_1.jpeg" style="position:absolute;top:0;left:0;" /> -->
  <!-- <img class="fragment" src="img/AdobeStock_628760640_2.png" style="position:absolute;top:0;left:0;" /> -->
</div>

<!-- ![](img/AdobeStock_628760640.jpeg){width="55%" height="30%" fig-align="center"} -->


<!-- ::: fragment
$P(\text{arrive on time} | rain) = \frac{P(rain | \text{arrive on time})P(\text{arrive on time})}{P(rain)}$
::: -->

<!-- ## Bayes rule

<br> <br>

More formally, Bayes rule says:

<div class="r-stack" style="position:relative; width:640px; height:480px; margin:0 auto;">
  <img class="fragment current-visible" src="img/AdobeStock_628760640.jpeg" style="position:absolute;top:0;left:0;">
  <img class="fragment current-visible" src="img/AdobeStock_628760640_1.jpeg" style="position:absolute;top:0;left:0;">
  <img class="fragment" src="img/AdobeStock_628760640_2.png" style="position:absolute;top:0;left:0;">
</div> -->

## Bayes rule
#### An example 



::: incremental
-   **My prior**: 90% of EPFL employees are on time

-   **New knowledge**: a speaker at the annual meeting will be from EPFL Biotech

-   What is my **posterior**? what is the probability that the speaker will be on time?
:::

<div style="position:relative; width:640px; height:480px; margin:0 auto;">
  <img class="fragment" src="img/bayes-example1.png"   style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/bayes-example2.png" style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/bayes-example3.png" style="position:absolute;top:0;left:0;" />
</div>

<!-- ![](img/bayes-example1.jpg){.absolute .fragment width="70%" height="70%" fig-align="center"}

![](img/bayes-example2.jpg){.absolute .fragment width="70%" height="70%" fig-align="center"}

![](img/bayes-example3.jpg){.absolute .fragment width="70%" height="70%" fig-align="center"}
 -->

::: notes
posterior p(speaker is on time \| ny office) or posterior p(wef on time \| ny office)?
:::

## Bayes rule
#### An example 

<br> <br>
<!-- ![](img/bayes-example3.jpg){width="60%" height="60%" fig-align="right"} -->

::: columns
::: {.column width="40%" }

::: {style="font-size:16px"}
-  **Prior**: $P(\text{on time})$
-  **Likelihood**: $P(\text{Biotech}|\text{on time})$
-  **Marginalization**: $P(\text{Biotech})$
:::


::: {style="font-size:16px"}
\begin{align}
P(\text{on time}|\text{Biotech})& = \frac{P(\text{Biotech}|\text{on time})\times P(\text{on time})}{P(\text{Biotech})} \\
& = \frac{0.4 \times 0.9}{0.4 \times 0.9 + 0.8 \times 0.1} = 0.82
\end{align}
:::

:::

::: {.column width="60%" }
![](img/bayes-example3.png){width="80%" height="80%" fig-align="right"}
:::
:::



<!-- ::: {style="font-size:16px"}
::: fragment
\begin{align}
P(\text{on time}|\text{NY office})& = \frac{P(\text{NY office}|\text{on time})\times P(\text{on time})}{P(\text{NY office})} \\
& = \frac{0.4 \times 0.9}{0.4 \times 0.9 + 0.8 \times 0.1} = 0.82
\end{align}
:::
::: -->

::: fragment
> My prior has dropped from **90% to 82%**
:::

::: notes
press c to activate the chalk and circle on the prior 90% and posterior 82% press c again to turn it off
:::


## How to obtain the posterior?

<br> 

We are usually interested in the full posterior **distribution** and not only in the **point** estimates. 

Remember the Bayes rule:

<div style="position:relative; width:640px; height:300px; margin:0 auto;">
  <img class="fragment" src="img/AdobeStock_628760640.jpeg"   style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/AdobeStock_628760640_1.jpg" style="position:absolute;top:0;left:0;" />
  <img class="fragment" src="img/AdobeStock_628760640_2.jpg" style="position:absolute;top:0;left:0;" />
</div>

. . .

Let's rewrite it as:

$$\color{#962cff}{P(\theta|data)} \propto \color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}$$

Let's check each of this terms: what are the likelihood and prior? what is $\theta$?

## How to obtain the posterior?
#### What is $\theta$?

Consider the example of flipping a coin. Then $\theta$ is the **probability** of getting a head. 

<br>

Other real-world examples are the probability that:

<br>

- a person has a disease
- a person buys a product
- a person clicks on an ad
- a person defaults on a loan
- a person is left handed
- a free throw by a basketball player is successful
- a baby is a girl
- a widget on an assembly line is faulty


## How to obtain the posterior?
#### Step back: what's the big picture?

We are usually interested in the full posterior **distribution** and not only in the **point** estimates. There are at least two approaches to obtain the distribution:

::: fragment

> [**Analytical Methods**:]{style="color: #6ebef0;"}

- They are based on `exact` formulas and conjugate prior concept
- Examples are Beta and Gamma distributions
- We introduce these methods and will use them in the hands-on session today
:::

::: fragment

> [**Numerical Methods**:]{style="color: #6ebef0;"}

- They are based on Markov Chain Monte Carlo `sampling` methods
- Examples are Metropolis-Hastings, Gibbs, and No-U-Turn Sampler (NUTS) methods
- We will use these techniques in the hands-on sessions tomorrow
:::

::: notes
In Bayesian inference, we are interested in the posterior **distribution** of the parameters of interest, and not only in the **point** estimates.
So, question is how to obtain the posterior distribution? 
:::


## Analytical Approach
#### Objective: find out the posterior given a prior and likelihood

<br>

Here is what we want to do: 
<!-- [violet]{style="color:#962cff"} <-- [blue]{style="color:blue"} + [red]{style="color:red"}  -->

- We begin with some [prior]{style="color:blue"} distribution of $\theta$
- We observe some data from the results of flipping the coin. So we can find the [likelihood]{style="color:red"} of the data given the parameter $\theta$.
- We infer the [posterior]{style="color:#962cff"} distribution using Bayes’ rule

<br>
<!-- $$\color{#962cff}{posterior} : \frac{\color{red}{likelihood}\times\color{blue}{prior}}{\color{red}{evidence}}$$

or simply, $\color{#962cff}{posterior} \propto \color{red}{likelihood}\times\color{blue}{prior}$

<br>


Mathematically: -->

<!-- $$\color{#962cff}{P(\theta|data)} = \frac{\color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}}{\color{red}{P(data)}}$$ -->

$$\color{#962cff}{P(\theta|data)} \propto \color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}$$


::: notes
The “likelihood” p(D|θ), is the probability that the data could be generated by the model with parameter value θ.
The “evidence” for the model, p(D), is the overall probability of the data according to the model, averaged over all possible parameter values. It is the normalization constant that ensures that the posterior is a proper probability distribution.
The term “evidence” is in fact the “marginal likelihood” that refers specifically to the operation of taking the average of the likelihood, p(D|θ), across all values of θ, weighted by the prior probability of θ.
:::

## Analytical Approach
#### Likelihood ... but what is the likelihood?

<br>

Let's go back to the coin example:

The probability of each outcome can be calculated directly from the Bernoulli distribution which has the form:

$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

where $x$ is the outcome of the coin flip (0 or 1) and $\theta$ is the probability of getting a head, and can be fixed to be 0.5, for example.

<br>

So, $P(x|\theta=0.5)$ is only a function of $x$.

## Analytical Approach
#### Likelihood  ... but what if we fix $x$ and let $\theta$ vary?

<br>

In this case we get a function of $\theta$ that is called the (Bernoulli) likelihood function for $\theta$:

$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

<br>

If we flip the coin N times and get $z=\sum_{i}x_i$  heads, then the likelihood function is:

$$P(x|\theta) = \theta^z(1-\theta)^{N-z}$$

Now we need to specify the prior: 

- Likelihood ✅
- prior ❓
- posterior ❓

<!-- $$\color{#962cff}{posterior} \propto \color{red}{likelihood}\times\color{blue}{prior}$$ -->

## Analytical Approach
#### Prior  ... what should be the prior?

<br>

- The prior $P(\theta)$ can be any function that gives probability for the values of $0 \leq \theta \leq 1$.

- For example, we can choose a uniform prior, which means that all values of $\theta$ are equally likely. There are many other choices.

- In Statistics, there are many instance where the choice is simply a matter of convenience. That's the case here for the prior:

- Beta distribution as a prior is a convenient choice because it is a **conjugate prior** to the Bernoulli likelihood function.

<br>


::: callout-tip

<!-- [prior]{style="color:red"} ~ Beta distribution $\Rightarrow$  [posterior]{style="color:#962cff"} ~ Beta distribution. -->

When the prior and posterior are in the same family of distributions, we say that the prior is a conjugate prior to the likelihood function. Or both prior and posterior are conjugate distributions.
:::

::: notes
explain why we are focusing on the beta distribution as a prior. we are interested in the conversion probability, which is a probability, so we need a distribution that is defined on the interval [0,1]. Beta distribution is a convenient choice because it is a conjugate prior to the Bernoulli likelihood function.
:::

## Analytical Approach{data-visibility="hidden"}
#### Prior  ... what is Beta distribution?

<br>


Let's see what the Beta distribution looks like:

$$P(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$

where $B(\alpha,\beta)$ is the beta function:

$$B(\alpha,\beta) = \int_0^1 \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta$$

and $\alpha>0$ and $\beta>0$ are the shape parameters of the distribution, and $0 \leq \theta \leq 1$.

## Analytical Approach
#### Prior  ... what is Beta distribution?

<br>
$$P(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}, \hspace{20 mm} \alpha, \beta>0, 0 \leq \theta \leq 1$$

Let's play with the [Beta distribution](https://shinylive.io/py/app/#code=NobwRAdghgtgpmAXGKAHVA6VBPMAaMAYwHsIAXOcpMAHQgEsZViAnMgAgGcALeibTgHcoZQtzqNmbdjBGoANsTLz6AIyzYFS9lE7sFZCU1YcIAVybYdeiKjoAzFsRhde-dpJPsAgujzsWSgATOBZ-M3oHJxdOQnocDE4yET1PaVU4ZLo6NFQAfQj2AF52CKwoAHM4PPt5CKCACjp2Ftc+AWFRbgwybjh4RIBrTLFsBoBKPGbWsvkobGIzMjzOehDVKBYmiFbd0vpyiDh5FbW4Da3pvev2AGJ2ABV6ZTgrm5ay7gAWJrAACWIgnYQWIcD0qE2sEyoT0xHs7F6cHYACFMlBgfQkiw1Et6KR2GIoBAqh4yHoeGg4AB+GhgSZvd73PioJZcFQhFgMm5lZlLFbs0K-KC0-y0gAKkPgFBYOhF7AAjP55QAGPDy+k7d4zA685acAWXMCqOXiyXQmXG-AKpWq9VTTVa-ZYIXsAB87FV7FUbo9tI1XJaGveZQhRxOsj420dHwOizILOWWjIv2g8DyqCC9j99veQdaGvG2QgdBC8M4oQAboLdf44wn-OXOKtSONEG8AAJ1pYdwIQDlYRTJqDyMhFWnedhigAiADE-W9S+xU9UM-YJm2HXtbBgWESQTBEnA4I15QBOABsAA5ldf1QG7gAC8scREBIkk5UAHiKAA9v-L7x-Yol0wFQIE4CFCDgBplQwZVFVg09TzwABWZVC03XYDAHJQGh-fwMmSLBMzw-xdQwKAJjwcjVAmSZ2AAckGABaBi8HkQQigAJgw95sLIZ55GghjUWSfQnA2VR6BUMgrBCcDnisewzAgQgBNIBjeJubCfzmDJ5AaFgGIAEhoGhEWSYzNPvbDsD045DJMsUmnMvpLPGazMNabChKqPsGkUQgigYjIkjY9hHChUgihnYdywwmyR0SbhAQmIscnQYDfFQBpcgKegG0rUJ-HWMwKiKB4WDMOBxjAABfPBwGXahAgARwiQJ4HITgeh-Mh8CIUgKCoZAeHaIQRDEeqAF0gA) and see how it changes with $\alpha$ and $\beta$.

<br>

::: info-box
Beta distribution can become flat, symmetric or asymmetric depending on the values of $\alpha$ and $\beta$
:::

<!-- ::: fragment
Observations:
::: -->

<!-- ::: incremental
- Uniform distribution is a special case of the Beta distribution with $\alpha=\beta=1$
- Beta distribution can become symmetric, e.g. with $\alpha=\beta=4$
- Beta distribution can become right (or left)-skewed, e.g. with $\alpha=2$ (for $\beta=5$)
::: -->

<br>

::: fragment
We're almost there: 

- Likelihood ✅
- prior ✅
- posterior ❓
:::

::: notes
note that changing the values of $\alpha$ and $\beta$ changes the shape of the distribution
:::

## Analytical Approach
#### Posterior  ... what would it be like if the prior is Beta? {.smaller}

<br>

![](img/posterior.png){width="40%" height="40%" fig-align="center"}

$$\color{#962cff}{P(\theta|data)} \propto \color{red}{\theta^z(1-\theta)^{N-z}}\times\color{blue}{\theta^{\alpha-1}(1-\theta)^{\beta-1}}$$

<!-- $\color{#962cff}{P(\theta|data)} \propto \theta^{z+\alpha-1}(1-\theta)^{N-z+\beta-1}$ -->
::: fragment

$$\color{#962cff}{Beta(\alpha+z,\beta+N-z)} \propto \color{red}{\text{Bernoulli likelihood (N, z, $\theta$)}} \times\color{blue}{Beta(\alpha,\beta)}$$
:::

<br>

::: fragment
Let's have a little [show](https://shinylive.io/py/app/#code=NobwRAdghgtgpmAXGKAHVA6VBPMAaMAYwHsIAXOcpMAHQgEsZViAnMgAgGcALeibTgHcoZQtzqNmbdjBGoANsTLz6AIyzYFS9lE7sFZCU1YcIAVybYdeiKjoAzFsRhde-dpJPsAgujzsWSgATOBZ-M3oHJxdOQnocDE4yET1PaVU4ZLo6NFQAfQj2AF52CKwoAHM4PPt5CKCACjp2Ftc+AWFRbgwybjh4RIBrTLFsBoBKPGbWsvkobGIzMjzOehDVKBYmiFbd0vpyiDh5FbW4Da3pvev2AGJ2ABV6ZTgrm5ay7gAWJrAACWIgnYQWIcD0vTg7AAQpkoPpiElQvRWOwxFAIFU9KorBD9CxkSwdBAgtDQhBFvIVOwVMMVNxiMQggB+GhgSZvd73PioJZcFQhFgcm5lblLFb80K-KCs-ysgAKm1gmVCOhl7AAjP51QAGPDq9k7d4zA6i5acCWXMCqNXyxXwCiE634DVa3X6qaGo37DCm8VnS0AORtkAsGUJxHs7Fq8U4ap1rrd2oNXo+JtsYvN-t+EDyfSgQVjztZ5hgYfYEfYeYLat1Gu1ieTKbKqF+rINQpajeuzfRxzysj42xT3sWZB5yy0ZGzSryqCC9jbHveXfYBvG2QgdBCkc4oQAbpLTf5R+P-LvOKtSONEG8AAInpZ3wLE0JYRRTqDyMhFVnedhygARAAxNs3m3dhoHgWd5wmG9PT2WwMBYdEQRgRI4DgRp1QATgANgADm1Qj9Q7dhSNuAAC3cOFxZCMUhbUAB4ilQZj1VIiFkmKCDMBUCBOFQKBCDgBptQwbVNTE7DsL1et103eDdnuQhAhESEyEEYh9HfVIdjhJxBFI+x6AqfwoAADzBbiDESMxVEnTgGk1dgACZ-GMipVgALzgIonLcr5xnkzltO0GEWHJMxKXoal6Fpeh6UZUiLLBYBtQAXTfJQGk4qB-FygAqArTQwHMq0c8Z2AK9gnIAWly8YioaEqAwmWqSrKuB8wqyZ2AAchYPr-BgTgihc4KbmSyzODSzLqLycy5gyeQGkGgASGgaFyta+om64UpmjL0OWbAluOBo+ppY4EoZIJdqm1KjvmshnnkES+vCyLoti+LEruva9luA7ZowCp8UaAHdnI0KOFQJwNlUegVDIKwgnoJJ8VUJZkQU95gfVTLJxyvpkn8DJkiwGDcv8EqoAmPAStUCZer61RaqG6koGWoo+rhgl-2Aob5EEMbIdafHCffYnYTJ2FKfsaXSY8dMyAwOnxgAag63MuoLenlfHDAmc1lq2u18rmf8Vm6jgPdkTesgOeFsazPkVBuCgIoxNw-wzvkHnmERfEUUAoD7sU1oHpmgnjoWv3Vr6jatpJqAdrFloJdj06ufOvrw7x6bgBj57XvevmURQ+Eg-50POHzm5M7eqpiQaRRCB5jIkg5xwlVIIogM-Xd07uTOwbWCZSNImyXoqbhljmBYlgniOWhsnhAWXjdcm43wW1yAp6DPfdQn8dYzAqIoHhYMw4HGMAAF88HASCEGQQIAEcIkCeByE4HpzLIPgIgpAKBUGQDwdoQgRBiAfulIAA) of this.

:::

::: notes
a few things we learn from the plot:
- as you noted in the plots, posterior is a compromise between the prior and the likelihood
- the more data we have, the more the posterior is dominated by the likelihood, and less by the prior
- the posterior is a probability distribution, so it integrates to 1
- the posterior is a Beta distribution with parameters $\alpha+z$ and $\beta+N-z$
- a+b=N
:::

<!-- ## Analytical Approach
#### Prior  ... what is Gamma distribution?

<br>


Similar to beta distribution, gamma distribution is a convenient choice for the prior because it is a **conjugate prior** to the Gamma likelihood function. -->

## Hands-on session{data-visibility="hidden"}
#### Warm-up

<br>

::: panel-tabset

### code
```{.python code-line-numbers="9|3|6"}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import beta

# create a range of x values
x = np.linspace(0,1,100)

# calculate the probability density function for x values
beta_pdf = beta.pdf(x, a=1, b=1)

# plot probability distribution function
plt.plot(x, beta_pdf)
plt.xlabel('x values')
plt.ylabel('f(x)')
plt.title('Beta distribution')
plt.show()F
```
### plot
```{python}
#| echo: false
#| eval: true
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import beta

# create a range of x values
x = np.linspace(0,1,100)

# calculate the probability density function for x values
beta_pdf = beta.pdf(x, a=1, b=1)

# plot probability distribution function
plt.plot(x, beta_pdf)
plt.xlabel('x values')
plt.ylabel('f(x)')
plt.title('Beta distribution')
plt.show()
```
:::


## {chalkboard-buttons="false" background-image="../assets/img/Handson_AdobeStock_316152114.jpeg" background-position="center"  background-repeat="no-repeat" background-opacity="0.5" background-size="cover" }

![](../assets/img/python-logo-inkscape.svg){width="30%" height="30%" fig-align="center"}

Let's begin:

-   Open the Jupyter notebook in `Day1-03/Handson.ipynb`
- Run the codes and provide your answers to the tasks in the notebook
-   You will be able to find the solution notebook by the end of the day
- [Please be back here at 14:00]{style="color:#ff412c; font-weight: bold;"}


::: footer
:::

## {chalkboard-buttons="false" background-image="../assets/img/Coffee_AdobeStock_318442666.jpeg" background-size="99.8%" background-opacity="0.8" }

::: footer

:::


<!-- ## {chalkboard-buttons="false" background-iframe="https://extensionschool.ch" background-interactive="true" background-interactive-end="true" background-interactive-start="true" background-opacity="0.7" background-position="center" } 


::: footer
::: -->
