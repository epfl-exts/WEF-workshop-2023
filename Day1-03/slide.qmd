---
title: Bayesian Statistics
subtitle: Bayes Rule and Bayesian Inference
author: EPFL Extension School
institute: World Economic Forum
date: 2023-09-28
editor:
  render-on-save: false
format:
  revealjs:
    code-copy: true
    code-link: true
    embed-resources: false
    smaller: true
    scrollable: false
    theme: ../assets/theme.scss 
    logo: ../assets/img/logo_red.png
    code-fold: true
    code-line-numbers: false
    slide-number: true
    chalkboard:
      buttons: true
    preview-links: auto
    menu: false
from: markdown+emoji
filters:
  - shinylive
execute:
  enabled: true
  cache: true
jupyter: python3

---


## {chalkboard-buttons="false" background-image="../assets/img/AdobeStock_536776996.jpeg"  background-size="cover"}

 
[Goal of This Session:]{style="font-size: 1.5em; font-weight: bold;"}

<br>


:heavy_check_mark: Python

<br>

:heavy_check_mark: Statistical Distributions & Inference

<br>

&#9744; [Bayesian Statistics]{style="color:#ff412c;"}


## Bayes rule 

<br> <br>

::: columns
::: {.column width="50%"}
::: fragment
![](img/AdobeStock_228399688.jpeg){width="40%" height="40%" fig-align="center"}
:::
:::

::: {.column width="50%"}
::: incremental
-   `P(arrive on time) = 0.9`

<br>

-   `P(late) = 0.1`
:::
:::
:::

. . .

::: columns
::: {.column width="50%"}
-   That's our perception about how likely these two events are

-   Let's call that our [prior]{style="color:#ff412c"} knowledge about the events
:::

::: {.column width="50%"}
![](img/AdobeStock_607202393.jpeg){width="50%" height="50%" fig-align="center"}
:::
:::

::: notes
Suppose that the probability that one arrives on time to work is 90%. The only other outcome is that they arrive late These two probabilities show how we perceive the two events and their associated possibilities, prior to gaining further knowledge about them.
:::

## Bayes rule

<br> <br>

**But ...** 



::: columns
::: {.column width="50%"}
-   What if we start learning more about the 'situation'?
-   Should we stick to our [prior]{style="color:#ff412c"} idea about the events and their possibilities?
:::

::: {.column width="50%"}
![](img/AdobeStock_462364017.jpeg){width="50%" height="50%" right="180" fig-align="center"}
:::
:::

. . .

If not, how we should adapt our priors to the new situation?

. . .


::: goal
This is where Bayesian Statistics steps in and provide answers
:::

## Bayes rule {.smaller}

<br> Byes rule provides a simple passage from our [prior]{style="color:#ff412c"} idea to [new insight]{style="background-color: yellow;"}

::: fragment
![](img/bayes%20-%20rule.png){width="70%" height="70%" fig-align="center"}
:::

::: fragment
![](img/bayes%20-%20rule%20-%20example.jpg){width="70%" height="70%" fig-align="center"}
:::

. . .

Let's call the new idea a [posterior]{style="background-color: yellow;"}

. . .

`P(arrive on time)` → `P(arrive on time | it’s raining)` 

::: notes
press c to activate the chalk and circle on the P(arrive on time \| it's raining) and the word posterior press c again to turn it off
:::

## Bayes rule

<br> <br>

More formally, Bayes rule says:



![](img/AdobeStock_628760640.jpeg){width="55%" height="30%" fig-align="center"}


::: fragment
$P(\text{arrive on time} | rain) = \frac{P(rain | \text{arrive on time})P(\text{arrive on time})}{P(rain)}$
:::


## Bayes rule: an example



::: incremental
-   **My prior**: 90% of WEF employees are on time

-   **Evidence**: a speaker at the annual meeting will be from WEF NY office

-   What is my **posterior**? what is the probability that the speaker will be on time?
:::





![](img/bayes-example1.jpg){.absolute .fragment width="70%" height="70%" }

![](img/bayes-example2.jpg){.absolute .fragment width="70%" height="70%" }

![](img/bayes-example3.jpg){.absolute .fragment width="70%" height="70%" }


::: notes
posterior p(speaker is on time \| ny office) or posterior p(wef on time \| ny office)?
:::

## Bayes rule: an example {.smaller}



![](img/bayes-example3.jpg){width="60%" height="60%" fig-align="center"}

::: {style="font-size:14px"}
::: fragment
```{=tex}
\begin{align}
P(\text{on time}|\text{NY office})& = \frac{P(\text{NY office}|\text{on time})\times P(\text{on time})}{P(\text{NY office})} \\
& = \frac{0.4 \times 0.9}{0.4 \times 0.9 + 0.8 \times 0.1} = 0.82
\end{align}
```
:::
:::

. . .

my prior has dropped from **90% to 82%**


::: notes
press c to activate the chalk and circle on the prior 90% and posterior 82% press c again to turn it off
:::

## How to obtain the posterior?


We are usually interested in the full posterior **distribution** and not only in the **point** estimates. There are at least two approaches to obtain the distribution:


> [**Analytical Methods**:]{style="color: #6ebef0;"}

- They are based on `exact` formulas and conjugate prior concept
- Examples are Beta and Gamma distributions
- We introduce these methods and will use them in the hands-on session today

. . . 

> [**Numerical Methods**:]{style="color: #6ebef0;"}

- They are based on Markov Chain Monte Carlo `sampling` methods
- Examples are Metropolis-Hastings, Gibbs, and No-U-Turn Sampler (NUTS) methods
- We will use these techniques in the hands-on sessions tomorrow


::: notes
In Bayesian inference, we are interested in the posterior **distribution** of the parameters of interest, and not only in the **point** estimates.
So, question is how to obtain the posterior distribution? 
:::


## Analytical Approach

Consider the example of flipping a coin, but other real-world examples include:

The probability that:

- a person has a disease
- a person buys a product
- a person clicks on an ad
- a person defaults on a loan
- a person is left handed
- a free throw by a basketball player is successful
- a baby is a girl
- a widget on an assembly line is faulty

## Analytical Approach
#### Objective: find out the posterior given a prior and likelihood
Here is what we want to do: 
<!-- [violet]{style="color:#962cff"} <-- [blue]{style="color:blue"} + [red]{style="color:red"}  -->

- we begin with some [prior]{style="color:blue"} distribution of the outcome of coin flips
- we observe some [data]{style="color:red"} from the results of flipping the coin
- we infer the [posterior]{style="color:#962cff"} distribution of credibility using Bayes’ rule

$$\color{#962cff}{posterior} : \frac{\color{red}{likelihood}\times\color{blue}{prior}}{\color{red}{evidence}}$$

or simply, $\color{#962cff}{posterior} \propto \color{red}{likelihood}\times\color{blue}{prior}$

<br>


Mathematically:

<!-- $$\color{#962cff}{P(\theta|data)} = \frac{\color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}}{\color{red}{P(data)}}$$ -->

$$\color{#962cff}{P(\theta|data)} \propto \color{red}{P(data|\theta)}\times\color{blue}{P(\theta)}$$


::: notes
The “likelihood” p(D|θ), is the probability that the data could be generated by the model with parameter value θ.
The “evidence” for the model, p(D), is the overall probability of the data according to the model, averaged over all possible parameter values. It is the normalization constant that ensures that the posterior is a proper probability distribution.
The term “evidence” is in fact the “marginal likelihood” that refers specifically to the operation of taking the average of the likelihood, p(D|θ), across all values of θ, weighted by the prior probability of θ.
:::

## Analytical Approach
#### Likelihood ... but what is the likelihood?

let's go back to the coin example:

the probability of each outcome can be calculated directly from the Bernoulli distribution which has the form:

$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

where $x$ is the outcome of the coin flip (0 or 1) and $\theta$ is the probability of getting a head, and can be fixed to be 0.5, for example.

So, $P(x|\theta=0.5)$ is only a function of $x$.

## Analytical Approach
#### Likelihood  ... but what if we fix $x$ and let $\theta$ vary?

In this case we get a function of $\theta$ that is called the (Bernoulli) likelihood function for $\theta$:

$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

if we flip the coin N times and get $z=\sum_{i}x_i$  heads, then the likelihood function is:

$$P(x|\theta) = \theta^z(1-\theta)^{N-z}$$

Now we only need to specify the prior: 

$$\color{#962cff}{posterior} \propto \color{red}{likelihood}\times\color{blue}{prior}$$

## Analytical Approach
#### Prior  ... what should be the prior?

The prior $P(\theta)$ can be any function that gives probability for the values of $0 \leq \theta \leq 1$.

For example, we can choose a uniform prior, which means that all values of $\theta$ are equally likely. There are many other choices.

In Statistics, there are many instance where the choice is simply a matter of convenience. That's the case here for the prior:

The prior $P(\theta)$ ~ Beta distribution is a convenient choice because it is a **conjugate prior** to the Bernoulli likelihood function.

Conjugate prior means that the posterior distribution is of the same form as the prior distribution:

[prior]{style="color:red"} ~ Beta distribution $\rightarrow$ [posterior]{style="color:#962cff"} ~ Beta distribution

## Analytical Approach
#### Prior  ... what is Beta distribution?

Let's see what the Beta distribution looks like:

$$P(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$

where $B(\alpha,\beta)$ is the beta function:

$$B(\alpha,\beta) = \int_0^1 \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta$$

and $\alpha>0$ and $\beta>0$ are the shape parameters of the distribution, and $0 \leq \theta \leq 1$.

## Analytical Approach
#### Prior  ... what is Beta distribution?

Let's play with the [Beta distribution](https://shinylive.io/py/app/#code=NobwRAdghgtgpmAXGKAHVA6VBPMAaMAYwHsIAXOcpMAHQgEsZViAnMgAgGcALeibTgHcoZQtzqNmbdjBGoANsTLz6AIyzYFS9lE7sFZCU1YcIAVybYdeiKjoAzFsRhde-dpJPsAgujzsWSgATOBZ-M3oHJxdOQnocDE4yET1PaVU4ZLo6NFQAfQj2AF52CKwoAHM4PPt5CKCACjp2Ftc+AWFRbgwybjh4RIBrTLFsBoBKPGbWsvkobGIzMjzOehDVKBYmiFbd0vpyiDh5FbW4Da3pvev2AGJ2ABV6ZTgrm5ay7gAWJrAACWIgnYQWIcD0qE2sEyoT0xHs7F6cHYACFMlBgfQkiw1Et6KR2GIoBAqh4yHoeGg4AB+GhgSZvd73PioJZcFQhFgMm5lZlLFbs0K-KC0-y0gAKkPgFBYOhF7AAjP55QAGPDy+k7d4zA685acAWXMCqOXiyXQmXG-AKpWq9VTTVa-ZYIXsAB87FV7FUbo9tI1XJaGveZQhRxOsj420dHwOizILOWWjIv2g8DyqCC9j99veQdaGvG2QgdBC8M4oQAboLdf44wn-OXOKtSONEG8AAJ1pYdwIQDlYRTJqDyMhFWnedhigAiADE-W9S+xU9UM-YJm2HXtbBgWESQTBEnA4I15QBOABsAA5ldf1QG7gAC8scREBIkk5UAHiKAA9v-L7x-Yol0wFQIE4CFCDgBplQwZVFVg09TzwABWZVC03XYDAHJQGh-fwMmSLBMzw-xdQwKAJjwcjVAmSZ2AAckGABaBi8HkQQigAJgw95sLIZ55GghjUWSfQnA2VR6BUMgrBCcDnisewzAgQgBNIBjeJubCfzmDJ5AaFgGIAEhoGhEWSYzNPvbDsD045DJMsUmnMvpLPGazMNabChKqPsGkUQgigYjIkjY9hHChUgihnYdywwmyR0SbhAQmIscnQYDfFQBpcgKegG0rUJ-HWMwKiKB4WDMOBxjAABfPBwGXahAgARwiQJ4HITgeh-Mh8CIUgKCoZAeHaIQRDEeqAF0gA) and see how it changes with $\alpha$ and $\beta$.


A few observations:

::: incremental
- uniform distribution is a special case of the Beta distribution with $\alpha=\beta=1$
- it can become a symmetric distribution e.g. with $\alpha=\beta=4$
- it can become a right or left-skewed distribution e.g. with $\alpha=2$ and $\beta=5$
:::

::: notes
note that changing the values of $\alpha$ and $\beta$ changes the shape of the distribution
:::

## Analytical Approach
#### Posterior  ... what would it be like if the prior is Beta? {.smaller}

<br>

![](img/posterior.png){width="40%" height="40%" fig-align="center"}

$\color{#962cff}{P(\theta|data)} \propto \color{red}{\theta^z(1-\theta)^{N-z}}\times\color{blue}{\theta^{\alpha-1}(1-\theta)^{\beta-1}}$

$\color{#962cff}{P(\theta|data)} \propto \theta^{z+\alpha-1}(1-\theta)^{N-z+\beta-1}$

$\color{#962cff}{Beta(\alpha+z,\beta+N-z)} \propto \color{red}{\text{Bernoulli likelihood}} \times\color{blue}{Beta(\alpha,\beta)}$

<br>

Let's have a little [show](https://shinylive.io/py/app/#code=NobwRAdghgtgpmAXGKAHVA6VBPMAaMAYwHsIAXOcpMAHQgEsZViAnMgAgGcALeibTgHcoZQtzqNmbdjBGoANsTLz6AIyzYFS9lE7sFZCU1YcIAVybYdeiKjoAzFsRhde-dpJPsAgujzsWSgATOBZ-M3oHJxdOQnocDE4yET1PaVU4ZLo6NFQAfQj2AF52CKwoAHM4PPt5CKCACjp2Ftc+AWFRbgwybjh4RIBrTLFsBoBKPGbWsvkobGIzMjzOehDVKBYmiFbd0vpyiDh5FbW4Da3pvev2AGJ2ABV6ZTgrm5ay7gAWJrAACWIgnYQWIcD0vTg7AAQpkoPpiElQvRWOwxFAIFU9KorBD9CxkSwdBAgtDQhBFvIVOwVMMVNxiMQggB+GhgSZvd73PioJZcFQhFgcm5lblLFb80K-KCs-ysgAKm1gmVCOhl7AAjP51QAGPDq9k7d4zA6i5acCWXMCqNXyxXwCiE634DVa3X6qaGo37DCm8VnS0AORtkAsGUJxHs7Fq8U4ap1rrd2oNXo+JtsYvN-t+EDyfSgQVjztZ5hgYfYEfYeYLat1Gu1ieTKbKqF+rINQpajeuzfRxzysj42xT3sWZB5yy0ZGzSryqCC9jbHveXfYBvG2QgdBCkc4oQAbpLTf5R+P-LvOKtSONEG8AAInpZ3wLE0JYRRTqDyMhFVnedhygARAAxNs3m3dhoHgWd5wmG9PT2WwMBYdEQRgRI4DgRp1QATgANgADm1Qj9Q7dhSNuAAC3cOFxZCMUhbUAB4ilQZj1VIiFkmKCDMBUCBOFQKBCDgBptQwbVNTE7DsL1et103eDdnuQhAhESEyEEYh9HfVIdjhJxBFI+x6AqfwoAADzBbiDESMxVEnTgGk1dgACZ-GMipVgALzgIonLcr5xnkzltO0GEWHJMxKXoal6Fpeh6UZUiLLBYBtQAXTfJQGk4qB-FygAqArTQwHMq0c8Z2AK9gnIAWly8YioaEqAwmWqSrKuB8wqyZ2AAchYPr-BgTgihc4KbmSyzODSzLqLycy5gyeQGkGgASGgaFyta+om64UpmjL0OWbAluOBo+ppY4EoZIJdqm1KjvmshnnkES+vCyLoti+LEruva9luA7ZowCp8UaAHdnI0KOFQJwNlUegVDIKwgnoJJ8VUJZkQU95gfVTLJxyvpkn8DJkiwGDcv8EqoAmPAStUCZer61RaqG6koGWoo+rhgl-2Aob5EEMbIdafHCffYnYTJ2FKfsaXSY8dMyAwOnxgAag63MuoLenlfHDAmc1lq2u18rmf8Vm6jgPdkTesgOeFsazPkVBuCgIoxNw-wzvkHnmERfEUUAoD7sU1oHpmgnjoWv3Vr6jatpJqAdrFloJdj06ufOvrw7x6bgBj57XvevmURQ+Eg-50POHzm5M7eqpiQaRRCB5jIkg5xwlVIIogM-Xd07uTOwbWCZSNImyXoqbhljmBYlgniOWhsnhAWXjdcm43wW1yAp6DPfdQn8dYzAqIoHhYMw4HGMAAF88HASCEGQQIAEcIkCeByE4HpzLIPgIgpAKBUGQDwdoQgRBiAfulIAA) of this


::: notes
a few things we learn from the plot:
- as you noted in the plots, posterior is a compromise between the prior and the likelihood
- the more data we have, the more the posterior is dominated by the likelihood, and less by the prior
- the posterior is a probability distribution, so it integrates to 1
- the posterior is a Beta distribution with parameters $\alpha+z$ and $\beta+N-z$
- a+b=N
:::

## Analytical Approach
#### Prior  ... what is Gamma distribution?

similar to beta distribution, gamma distribution is a convenient choice for the prior because it is a **conjugate prior** to the Gamma likelihood function.

## Jupyter Notebook

<br>

::: panel-tabset

### code
```{.python code-line-numbers="9|3|6"}
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import beta

# create a range of x values
x = np.linspace(0,1,100)

# calculate the probability density function for x values
beta_pdf = beta.pdf(x, a=1, b=1)

# plot probability distribution function
plt.plot(x, beta_pdf)
plt.xlabel('x values')
plt.ylabel('f(x)')
plt.title('Beta distribution')
plt.show()
```
### plot
```{python}
#| echo: false
#| eval: true
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import beta

# create a range of x values
x = np.linspace(0,1,100)

# calculate the probability density function for x values
beta_pdf = beta.pdf(x, a=1, b=1)

# plot probability distribution function
plt.plot(x, beta_pdf)
plt.xlabel('x values')
plt.ylabel('f(x)')
plt.title('Beta distribution')
plt.show()
```
:::


## {chalkboard-buttons="false" background-image="../assets/img/Handson_AdobeStock_316152114.jpeg" background-position="center"  background-repeat="no-repeat" background-opacity="0.5" background-size="cover"}

![](../assets/img/python-logo-inkscape.svg){width="30%" height="30%" fig-align="center"}

Let's begin:

-   Open the Jupyter notebook in `Day1-03/hands-on.ipynb`
-   Follow the instructions in the notebook, and run the codes.
-   You will be able to find the solution to the exercise in the `Day1-03/hands-on-solution.ipynb` notebook by the end of the day.

::: footer
:::

## {chalkboard-buttons="false" background-image="../assets/img/Coffee_AdobeStock_318442666.jpeg" background-size="99.8%"}

::: footer

:::


<!-- ## {chalkboard-buttons="false" background-iframe="https://extensionschool.ch" background-interactive="true" background-interactive-end="true" background-interactive-start="true" background-opacity="0.7" background-position="center" } 


::: footer
::: -->
